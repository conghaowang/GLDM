{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!molecule_generation preprocess INPUT_DIR OUTPUT_DIR TRACE_DIR --generation-order=canonical\n",
    "Misc TODOs\n",
    "Add a motif embedding layer to the GNN => need to look into whether there are specific gnn layers for this\n",
    "Dataset => validate if it is correct\n",
    "Implement function for storing Data objects in the Pytorch geometric dataset\n",
    "Implement all the helper function for the pytorch geometric dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('trace_playground/metadata.pkl.gz', 'rb') as f:\n",
    "     metadata= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('trace_playground/train_0/train_0.pkl.gz', 'rb') as f:\n",
    "     train= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0a70c",
   "metadata": {},
   "source": [
    "# Misc TODOs\n",
    "\n",
    "1. Add a motif embedding layer to the GNN => need to look into whether there are specific gnn layers for this\n",
    "2. Dataset => validate if it is correct\n",
    "3. Implement function for storing Data objects in the Pytorch geometric dataset\n",
    "4. Implement all the helper function for the pytorch geometric dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86d30f",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The original dataset uses a function to randomly decide whether to include or exclude a particular partial graph from the batch. Instead of that, we try to simply save all Data objects a separate .pt files => at training time, we list all the file paths and then sample from the file paths uniformly \n",
    "\n",
    "- Downsides => will occupy more memory: potential optimisations (TODOs) would be to save as compressed file and only decompress at training time.\n",
    "- Upside => less preprocessing time at training time\n",
    "\n",
    "\n",
    "There will be 1 dataset object for each of the train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "import os\n",
    "class MolerDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        root, \n",
    "        raw_moler_trace_dataset_parent_folder, # absolute path \n",
    "        output_pyg_trace_dataset_parent_folder, # absolute path\n",
    "        split = 'train'\n",
    "        transform=None, \n",
    "        pre_transform=None, \n",
    "    ):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self._transform = transform \n",
    "        self._pre_transform = pre_transform\n",
    "        self._raw_moler_trace_dataset_parent_folder = raw_moler_trace_dataset_parent_folder\n",
    "        self._output_pyg_trace_dataset_parent_folder = output_pyg_trace_dataset_parent_folder\n",
    "        self._split = split\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"Raw generation trace files output from the preprocess function of the cli. These are zipped pickle\n",
    "        files.\"\"\"\n",
    "        raw_file_paths_folder = os.path.join(self._raw_moler_trace_dataset_parent_folder, self._split)\n",
    "        assert os.path.exist(raw_file_paths_folder), f'{raw_file_paths_folder} does not exist.'\n",
    "        raw_generation_trace_files = [os.path.join(file)  for file in os.listdir(raw_file_paths_folder)]\n",
    "        return raw_generation_trace_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"Processed generatio trace objects that are stored as .pt files\"\"\"\n",
    "        processed_file_paths_folder = os.path.join(self._output_pyg_trace_dataset_parent_folder, self._split)\n",
    "        assert os.path.exist(raw_file_paths_folder), f'{raw_file_paths_folder} does not exist.'\n",
    "        processed_generation_trace_files = [os.path.join(file)  for file in os.listdir(processed_file_paths_folder)]\n",
    "        return processed_generation_trace_files\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names_size(self):\n",
    "        return len(self.processed_file_names)\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Convert raw generation traces into individual .pt files for each of the trace steps.\"\"\"\n",
    "        # only call process if it was not called before\n",
    "        if self.processed_file_names_size > 0:\n",
    "            pass\n",
    "        else:\n",
    "            idx = 0\n",
    "            for pkl_file in self.raw_file_names:\n",
    "                # TODO refactor out this into a private function\n",
    "                with gzip.open(pkl_file, 'rb') as f:\n",
    "                     train= pickle.load(f)\n",
    "                \n",
    "                # Read data from `raw_path`.\n",
    "                data = Data(...)\n",
    "\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "                idx += 1\n",
    "\n",
    "    def _convert_raw_trace_steps_to_pickle(self, ):\n",
    "        with gzip.open(pkl_file, 'rb') as f:\n",
    "             train= pickle.load(f)\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_types_to_indices(self, node_types):\n",
    "    node_type_to_index\n",
    "    \"\"\"Convert list of string representations into list of integer indices.\"\"\"\n",
    "    return [node_type_to_index(node_type) for node_type in node_types]\n",
    "\n",
    "\n",
    "for generation_step in train_sample:\n",
    "    x = generation_step.partial_node_features\n",
    "    node_categorical_features = generation_step.node_categorical_features # TODO: do we need this?\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d053c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26374d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
