{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfaeb042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 12:03:39.670963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from dataset import MolerDataset, MolerData\n",
    "from utils import pprint_pyg_obj\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset = MolerDataset(\n",
    "    root = '/data/ongh0068', \n",
    "    raw_moler_trace_dataset_parent_folder = '/data/ongh0068/l1000/trace_playground',\n",
    "    output_pyg_trace_dataset_parent_folder = '/data/ongh0068/l1000/pyg_output_playground',\n",
    "    split = 'train',\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=False, follow_batch = [\n",
    "    'correct_edge_choices',\n",
    "    'correct_edge_types',\n",
    "    'valid_edge_choices',\n",
    "    'valid_attachment_point_choices',\n",
    "    'correct_attachment_point_choice',\n",
    "    'correct_node_type_choices',\n",
    "    'original_graph_x',\n",
    "    'correct_first_node_type_choices'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f05d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f33247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "params = {'full_graph_encoder': {'input_feature_dim': 32,\n",
    "  'atom_or_motif_vocab_size': 139},\n",
    " 'partial_graph_encoder': {'input_feature_dim': 32},\n",
    " 'mean_log_var_mlp': {'input_feature_dim': 832, 'output_size': 1024},\n",
    " 'decoder': {'node_type_selector': {'input_feature_dim': 1344,\n",
    "   'output_size': 140},\n",
    "  'node_type_loss_weights': torch.tensor([10.0000,  0.1000,  0.1000,  0.1000,  0.7879,  0.4924,  0.6060, 10.0000,\n",
    "           7.8786, 10.0000,  7.8786,  0.1000,  0.6565,  0.6565,  0.9848,  0.8754,\n",
    "           0.8754,  1.1255,  0.9848,  1.3131,  1.5757,  1.9696,  1.5757,  1.9696,\n",
    "           2.6262,  1.9696,  1.9696,  7.8786,  7.8786,  3.9393,  2.6262,  2.6262,\n",
    "           2.6262,  2.6262,  3.9393,  7.8786,  7.8786,  7.8786,  3.9393,  7.8786,\n",
    "          10.0000,  7.8786,  3.9393,  3.9393,  3.9393,  3.9393,  3.9393,  3.9393,\n",
    "           3.9393,  3.9393,  3.9393,  3.9393,  3.9393,  3.9393,  7.8786,  7.8786,\n",
    "          10.0000, 10.0000,  7.8786,  7.8786, 10.0000,  7.8786,  7.8786, 10.0000,\n",
    "           7.8786,  7.8786, 10.0000,  7.8786, 10.0000,  7.8786,  7.8786, 10.0000,\n",
    "           7.8786,  7.8786,  7.8786, 10.0000, 10.0000,  7.8786,  7.8786,  7.8786,\n",
    "           7.8786,  7.8786, 10.0000, 10.0000, 10.0000, 10.0000,  7.8786, 10.0000,\n",
    "          10.0000, 10.0000,  7.8786, 10.0000,  7.8786, 10.0000,  7.8786, 10.0000,\n",
    "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,  7.8786,\n",
    "          10.0000,  7.8786,  7.8786,  7.8786,  7.8786, 10.0000,  7.8786, 10.0000,\n",
    "          10.0000, 10.0000,  7.8786,  7.8786,  7.8786,  7.8786,  7.8786,  7.8786,\n",
    "           7.8786,  7.8786,  7.8786,  7.8786,  7.8786,  7.8786,  7.8786,  7.8786,\n",
    "           7.8786,  7.8786,  7.8786,  7.8786,  7.8786,  7.8786,  7.8786,  7.8786,\n",
    "           7.8786,  7.8786,  7.8786,  0.1000]),\n",
    "  'no_more_edges_repr': (1, 835),\n",
    "  'edge_candidate_scorer': {'input_feature_dim': 3011, 'output_size': 1},\n",
    "  'edge_type_selector': {'input_feature_dim': 3011, 'output_size': 3},\n",
    "  'attachment_point_selector': {'input_feature_dim': 2176, 'output_size': 1},\n",
    "  'first_node_type_selector': {'input_feature_dim': 832, 'output_size': 139}},\n",
    " 'latent_sample_strategy': 'per_graph',\n",
    " 'latent_repr_dim': 512,\n",
    " 'latent_repr_size': 512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717a6241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2168, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import BaseModel\n",
    "model = BaseModel(params, dataset).eval()\n",
    "\n",
    "moler_output = model._run_step(batch)\n",
    "\n",
    "loss = model.compute_loss(moler_output, batch)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(\n",
    "    graph_representations,\n",
    "    initial_molecules= None,\n",
    "    mol_ids = None,\n",
    "    store_generation_traces = False,\n",
    "    max_num_steps=120,\n",
    "    beam_size= 1,\n",
    "    sampling_mode = 'greedy',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd97f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_molecule_representations = model._full_graph_encoder(\n",
    "    original_graph_node_categorical_features=batch.original_graph_node_categorical_features,\n",
    "    node_features=batch.original_graph_x.float(),\n",
    "    edge_index=batch.original_graph_edge_index,\n",
    "    edge_type=batch.original_graph_edge_type.int(),\n",
    "    batch_index=batch.original_graph_x_batch,\n",
    ")\n",
    "\n",
    "\n",
    "partial_graph_representions, node_representations = model._partial_graph_encoder(\n",
    "    node_features=batch.x,\n",
    "    edge_index=batch.edge_index.long(),\n",
    "    edge_type=batch.edge_type.int(),\n",
    "    batch_index=batch.batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "912b00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply latent sampling strategy\n",
    "p, q, latent_representation = model.sample_from_latent_repr(\n",
    "    input_molecule_representations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f8d3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from molecule_generation.chem.molecule_dataset_utils import BOND_DICT\n",
    "from molecule_generation.chem.motif_utils import (\n",
    "    find_motifs_from_vocabulary,\n",
    ")\n",
    "uses_motifs = True\n",
    "initial_molecules = None\n",
    "mol_ids = None\n",
    "store_generation_traces = False\n",
    "\n",
    "graph_representations = latent_representation\n",
    "\n",
    "if initial_molecules is None:\n",
    "    initial_molecules = [None] * len(graph_representations)\n",
    "\n",
    "# Replace `None` in initial_molecules with empty molecules.\n",
    "initial_molecules = [\n",
    "    Chem.Mol() if initMol is None else initMol for initMol in initial_molecules\n",
    "]\n",
    "if mol_ids is None:\n",
    "    mol_ids = range(len(graph_representations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "567791cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from molecule_generation.utils.moler_decoding_utils import (\n",
    "    DecoderSamplingMode,\n",
    "    sample_indices_from_logprobs,\n",
    "    restrict_to_beam_size_per_mol,\n",
    "    MoLeRDecoderState\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "842578d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = []\n",
    "\n",
    "\n",
    "# preprocessing for when a scaffold is given\n",
    "for graph_repr, init_mol, mol_id in zip(graph_representations, initial_molecules, mol_ids):\n",
    "    num_free_bond_slots = [0] * len(init_mol.GetAtoms())\n",
    "\n",
    "    atom_ids_to_remove = []\n",
    "    atom_ids_to_keep = []\n",
    "\n",
    "    for atom in init_mol.GetAtoms():\n",
    "        if atom.GetAtomicNum() == 0:\n",
    "            # Atomic number 0 means a placeholder atom that signifies an attachment point.\n",
    "            bonds = atom.GetBonds()\n",
    "\n",
    "            if len(bonds) > 1:\n",
    "                scaffold = Chem.MolToSmiles(init_mol)\n",
    "                raise ValueError(\n",
    "                    f\"Scaffold {scaffold} contains a [*] atom with at least two bonds.\"\n",
    "                )\n",
    "\n",
    "            if not bonds:\n",
    "                # This is a very odd case: either the scaffold we got is disconnected, or\n",
    "                # it consists of just a single * atom.\n",
    "                scaffold = Chem.MolToSmiles(init_mol)\n",
    "                raise ValueError(f\"Scaffold {scaffold} contains a [*] atom with no bonds.\")\n",
    "\n",
    "            [bond] = bonds\n",
    "            begin_idx = bond.GetBeginAtomIdx()\n",
    "            end_idx = bond.GetEndAtomIdx()\n",
    "\n",
    "            neighbour_idx = begin_idx if begin_idx != atom.GetIdx() else end_idx\n",
    "            num_free_bond_slots[neighbour_idx] += 1\n",
    "\n",
    "            atom_ids_to_remove.append(atom.GetIdx())\n",
    "        else:\n",
    "            atom_ids_to_keep.append(atom.GetIdx())\n",
    "\n",
    "    if not atom_ids_to_remove:\n",
    "        # No explicit attachment points, so assume we can connect anywhere.\n",
    "        num_free_bond_slots = None\n",
    "    else:\n",
    "        num_free_bond_slots = [num_free_bond_slots[idx] for idx in atom_ids_to_keep]\n",
    "        init_mol = Chem.RWMol(init_mol)\n",
    "\n",
    "        # Remove atoms starting from largest index, so that we don't have to account for\n",
    "        # indices shifting during removal.\n",
    "        for atom_idx in reversed(atom_ids_to_remove):\n",
    "            init_mol.RemoveAtom(atom_idx)\n",
    "\n",
    "        # Determine how the scaffold atoms will get reordered when we canonicalize it, so we can\n",
    "        # permute `num_free_bond_slots` appropriately.\n",
    "        canonical_ordering = compute_canonical_atom_order(init_mol)\n",
    "        num_free_bond_slots = [num_free_bond_slots[idx] for idx in canonical_ordering]\n",
    "\n",
    "    # Now canonicalize, which renumbers all the atoms, but we've applied the same\n",
    "    # renumbering to `num_free_bond_slots` earlier.\n",
    "    init_mol = Chem.MolFromSmiles(Chem.MolToSmiles(init_mol))\n",
    "\n",
    "    # Clear aromatic flags in the scaffold, since partial graphs during training never have\n",
    "    # them set (however we _do_ run `AtomIsAromaticFeatureExtractor`, it just always returns\n",
    "    # 0 for partial graphs during training).\n",
    "    # TODO(kmaziarz): Consider fixing this.\n",
    "    Chem.Kekulize(init_mol, clearAromaticFlags=True)\n",
    "\n",
    "    init_atom_types = []\n",
    "    # TODO(kmaziarz): We need to be more careful in how the initial molecule looks like, to\n",
    "    # make sure that `init_mol`s have correct atom features (e.g. charges).\n",
    "    for atom in init_mol.GetAtoms():\n",
    "        init_atom_types.append(get_atom_symbol(atom))\n",
    "    adjacency_lists = [[] for _ in range(len(BOND_DICT))]\n",
    "    for bond in init_mol.GetBonds():\n",
    "        bond_type_idx = BOND_DICT[str(bond.GetBondType())]\n",
    "        adjacency_lists[bond_type_idx].append(\n",
    "            (bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())\n",
    "        )\n",
    "        adjacency_lists[bond_type_idx].append(\n",
    "            (bond.GetEndAtomIdx(), bond.GetBeginAtomIdx())\n",
    "        )\n",
    "\n",
    "    if uses_motifs:\n",
    "        init_mol_motifs = find_motifs_from_vocabulary(\n",
    "            molecule=init_mol, motif_vocabulary=dataset._motif_vocabulary\n",
    "        )\n",
    "    else:\n",
    "        init_mol_motifs = []\n",
    "\n",
    "    decoder_states.append(\n",
    "        MoLeRDecoderState(\n",
    "            molecule_representation=graph_repr,\n",
    "            molecule_id=mol_id,\n",
    "            molecule=init_mol,\n",
    "            atom_types=init_atom_types,\n",
    "            adjacency_lists=adjacency_lists,\n",
    "            visited_atoms=[atom.GetIdx() for atom in init_mol.GetAtoms()],\n",
    "            atoms_to_visit=[],\n",
    "            focus_atom=None,\n",
    "            # Pseudo-randomly pick last atom from input:\n",
    "            prior_focus_atom=len(init_atom_types) - 1,\n",
    "            generation_steps=[] if store_generation_traces else None,\n",
    "            motifs=init_mol_motifs,\n",
    "            num_free_bond_slots=num_free_bond_slots,\n",
    "        )\n",
    "    )\n",
    "\n",
    "decoder_states_empty = []\n",
    "decoder_states_non_empty = []\n",
    "\n",
    "for decoder_state in decoder_states:\n",
    "    if decoder_state.molecule.GetNumAtoms() == 0:\n",
    "        decoder_states_empty.append(decoder_state)\n",
    "    else:\n",
    "        decoder_states_non_empty.append(decoder_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6428f1",
   "metadata": {},
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d9355a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from decoder import MLPDecoder\n",
    "# from model_utils import get_params\n",
    "\n",
    "# decoder = MLPDecoder(get_params()['decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6f66aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoding_utils import sample_indices_from_logprobs\n",
    "import numpy as np\n",
    "\n",
    "decoder_states = decoder_states_empty\n",
    "\n",
    "# We only need the molecule representations.\n",
    "molecule_representations = torch.stack(\n",
    "    [state.molecule_representation for state in decoder_states]\n",
    ")\n",
    "\n",
    "first_node_type_logits = model.decoder.pick_first_node_type(\n",
    "    partial_graph_representions=partial_graph_representions\n",
    ")  # Shape [G, NT + 1]\n",
    "\n",
    "first_atom_type_logprobs = torch.nn.functional.log_softmax(\n",
    "    first_node_type_logits[:, 1:], dim=-1\n",
    ")  # Shape [G, NT]\n",
    "\n",
    "first_atom_type_pick_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e23c5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 138])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_atom_type_logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f5d4530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7a3de87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _decoder_pick_first_atom_types(\n",
    "    self,\n",
    "    *,\n",
    "    decoder_states: List[MoLeRDecoderState],\n",
    "    sampling_mode: DecoderSamplingMode,\n",
    "    num_samples: int = 1,\n",
    ") -> Iterable[Tuple[List[Tuple[Optional[str], float]], np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Query the model to pick the first atom to add for each of a list of decoder states.\n",
    "\n",
    "    Args:\n",
    "        decoder_states: MoLeRDecoderState objects representing partial\n",
    "            results of the decoder that are to be extended by an additional atom.\n",
    "        sampling_mode: Determines how to obtain num_samples. GREEDY takes the most\n",
    "            likely values, whereas SAMPLING samples according to the predicted\n",
    "            probabilities.\n",
    "        num_samples: Number of samples to return per input decoder state (non-1 values\n",
    "            are useful for beam search).\n",
    "\n",
    "    Returns:\n",
    "        A list of the same length as the input list `decoder_states`; the i-th entry\n",
    "        is the result for the i-th input.\n",
    "        A single result contains a list of `num_samples` first node type choices, and\n",
    "        an array containing all the first node type logprobs.\n",
    "    \"\"\"\n",
    "    if len(decoder_states) == 0:\n",
    "        return []\n",
    "\n",
    "    if not self.has_first_node_type_selector:\n",
    "        # Models without the first node type selector always start decoding from carbon.\n",
    "        def generate_result_that_picks_carbon():\n",
    "            first_node_type_picks = [(\"C\", 0.0) for _ in range(num_samples)]\n",
    "\n",
    "            # Probability of selecting anything other than carbon is 0.\n",
    "            first_node_type_logprobs = np.full(self._num_node_types, -BIG_NUMBER)\n",
    "            first_node_type_logprobs[self._carbon_type_idx] = 0.0\n",
    "\n",
    "            return first_node_type_picks, first_node_type_logprobs\n",
    "\n",
    "        return [generate_result_that_picks_carbon() for _ in range(len(decoder_states))]\n",
    "\n",
    "    # We only need the molecule representations.\n",
    "    molecule_representations = np.stack(\n",
    "        [state.molecule_representation for state in decoder_states]\n",
    "    )\n",
    "\n",
    "    first_node_type_logits = self.pick_first_node_type(\n",
    "        input_molecule_representations=molecule_representations, training=False\n",
    "    )  # Shape [G, NT + 1]\n",
    "\n",
    "    first_atom_type_logprobs = tf.nn.log_softmax(\n",
    "        first_node_type_logits[:, 1:], axis=1\n",
    "    ).numpy()  # Shape [G, NT]\n",
    "\n",
    "    first_atom_type_pick_results: List[\n",
    "        Tuple[List[Tuple[Optional[str], float]], np.ndarray]\n",
    "    ] = []\n",
    "\n",
    "    # Iterate over each of the rows independently, sampling for each input state:\n",
    "    for state_first_atom_type_logprobs in first_atom_type_logprobs:\n",
    "        picked_atom_type_indices = sample_indices_from_logprobs(\n",
    "            num_samples, sampling_mode, state_first_atom_type_logprobs\n",
    "        )\n",
    "\n",
    "        this_state_results: List[Tuple[Optional[str], float]] = []\n",
    "\n",
    "        for picked_atom_type_idx in picked_atom_type_indices:\n",
    "            pick_logprob: float = state_first_atom_type_logprobs[picked_atom_type_idx]\n",
    "            picked_atom_type_idx += 1  # Revert the stripping out of the UNK (index 0) type\n",
    "\n",
    "            this_state_results.append(\n",
    "                (self._index_to_node_type_map[picked_atom_type_idx], pick_logprob)\n",
    "            )\n",
    "\n",
    "        first_atom_type_pick_results.append(\n",
    "            (this_state_results, state_first_atom_type_logprobs)\n",
    "        )\n",
    "\n",
    "    return first_atom_type_pick_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9f579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 0: Pick first node types for states that do not have an initial molecule.\n",
    "first_node_pick_results = self._decoder_pick_first_atom_types(\n",
    "    decoder_states=decoder_states_empty, num_samples=beam_size, sampling_mode=sampling_mode\n",
    ")\n",
    "\n",
    "# print(\"I: Picked first node types:\", [picks for picks, _ in first_node_pick_results])\n",
    "\n",
    "decoder_states = decoder_states_non_empty\n",
    "\n",
    "for decoder_state, (first_node_type_picks, first_node_type_logprobs) in zip(\n",
    "    decoder_states_empty, first_node_pick_results\n",
    "):\n",
    "    for first_node_type_pick, first_node_type_logprob in first_node_type_picks:\n",
    "        # Set up generation trace storing variables, populating if needed.\n",
    "        atom_choice_info = None\n",
    "        if store_generation_traces:\n",
    "            atom_choice_info = MoleculeGenerationAtomChoiceInfo(\n",
    "                node_idx=0,\n",
    "                true_type_idx=None,\n",
    "                type_idx_to_prob=np.exp(first_node_type_logprobs),\n",
    "            )\n",
    "\n",
    "        new_decoder_state, added_motif = self._add_atom_or_motif(\n",
    "            decoder_state,\n",
    "            first_node_type_pick,\n",
    "            logprob=first_node_type_logprob,\n",
    "            choice_info=atom_choice_info,\n",
    "        )\n",
    "\n",
    "        last_atom_id = new_decoder_state.molecule.GetNumAtoms() - 1\n",
    "\n",
    "        if added_motif:\n",
    "            # To make all asserts happy, pretend we chose an attachment point.\n",
    "            new_decoder_state._focus_atom = last_atom_id\n",
    "\n",
    "        # Mark all initial nodes as visited.\n",
    "        new_decoder_state = MoLeRDecoderState.new_with_focus_marked_as_visited(\n",
    "            old_state=new_decoder_state, focus_node_finished_logprob=0.0\n",
    "        )\n",
    "\n",
    "        # Set the prior focus atom similarly to the start-from-scaffold case.\n",
    "        new_decoder_state._prior_focus_atom = last_atom_id\n",
    "\n",
    "        decoder_states.append(new_decoder_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_steps = 0\n",
    "while num_steps < max_num_steps:\n",
    "    # This will hold the results after this decoding step, grouped by input mol id:\n",
    "    new_decoder_states: List[MoLeRDecoderState] = []\n",
    "    num_steps += 1\n",
    "    # Step 1: Split decoder states into subsets, dependent on what they need next:\n",
    "    require_atom_states, require_bond_states, require_attachment_point_states = [], [], []\n",
    "    for decoder_state in decoder_states:\n",
    "        # No focus atom => needs a new atom\n",
    "        if decoder_state.focus_atom is None:\n",
    "            require_atom_states.append(decoder_state)\n",
    "        # Focus atom has invalid index => decoding finished, just push forward unchanged:\n",
    "        elif decoder_state.focus_atom < 0:\n",
    "            new_decoder_states.append(decoder_state)\n",
    "        else:\n",
    "            require_bond_states.append(decoder_state)\n",
    "\n",
    "    # Check if we are done:\n",
    "    if (len(require_atom_states) + len(require_bond_states)) == 0:\n",
    "        # print(\"I: Decoding finished\")\n",
    "        break\n",
    "\n",
    "    # Step 2: For states that require a new atom, try to pick one:\n",
    "    node_pick_results = self._decoder_pick_new_atom_types(\n",
    "        decoder_states=require_atom_states,\n",
    "        num_samples=beam_size,\n",
    "        sampling_mode=sampling_mode,\n",
    "    )\n",
    "\n",
    "    for decoder_state, (node_type_picks, node_type_logprobs) in zip(\n",
    "        require_atom_states, node_pick_results\n",
    "    ):\n",
    "        for node_type_pick, node_type_logprob in node_type_picks:\n",
    "            # Set up generation trace storing variables, populating if needed.\n",
    "            atom_choice_info = None\n",
    "            if store_generation_traces:\n",
    "                atom_choice_info = MoleculeGenerationAtomChoiceInfo(\n",
    "                    node_idx=decoder_state.prior_focus_atom + 1,\n",
    "                    true_type_idx=None,\n",
    "                    type_idx_to_prob=np.exp(node_type_logprobs),\n",
    "                )\n",
    "\n",
    "            # If the decoder says we need no new atoms anymore, we are finished. Otherwise,\n",
    "            # start adding more bonds:\n",
    "            if node_type_pick is None:\n",
    "                # print(I {decoder_state.molecule_id} {decoder_state.logprob:12f}: Finished decoding - p={node_type_logprob:5f}\")\n",
    "                new_decoder_states.append(\n",
    "                    MoLeRDecoderState.new_for_finished_decoding(\n",
    "                        old_state=decoder_state,\n",
    "                        finish_logprob=node_type_logprob,\n",
    "                        atom_choice_info=atom_choice_info,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                new_decoder_state, added_motif = self._add_atom_or_motif(\n",
    "                    decoder_state,\n",
    "                    node_type_pick,\n",
    "                    logprob=node_type_logprob,\n",
    "                    choice_info=atom_choice_info,\n",
    "                )\n",
    "\n",
    "                if added_motif:\n",
    "                    require_attachment_point_states.append(new_decoder_state)\n",
    "                else:\n",
    "                    require_bond_states.append(new_decoder_state)\n",
    "\n",
    "    if self.uses_motifs:\n",
    "        # Step 2': For states that require picking an attachment point, pick one:\n",
    "        require_attachment_point_states = restrict_to_beam_size_per_mol(\n",
    "            require_attachment_point_states, beam_size\n",
    "        )\n",
    "        (\n",
    "            attachment_pick_results,\n",
    "            attachment_pick_logits,\n",
    "        ) = self._decoder_pick_attachment_points(\n",
    "            decoder_states=require_attachment_point_states, sampling_mode=sampling_mode\n",
    "        )\n",
    "\n",
    "        for decoder_state, attachment_point_picks, attachment_point_logits in zip(\n",
    "            require_attachment_point_states,\n",
    "            attachment_pick_results,\n",
    "            attachment_pick_logits,\n",
    "        ):\n",
    "            for (attachment_point_pick, attachment_point_logprob) in attachment_point_picks:\n",
    "                attachment_point_choice_info = None\n",
    "                if store_generation_traces:\n",
    "                    attachment_point_choice_info = MoleculeGenerationAttachmentPointChoiceInfo(\n",
    "                        partial_molecule_adjacency_lists=decoder_state.adjacency_lists,\n",
    "                        motif_nodes=decoder_state.atoms_to_mark_as_visited,\n",
    "                        candidate_attachment_points=decoder_state.candidate_attachment_points,\n",
    "                        candidate_idx_to_prob=tf.nn.softmax(attachment_point_logits),\n",
    "                        correct_attachment_point_idx=None,\n",
    "                    )\n",
    "\n",
    "                # print(I {decoder_state.molecule_id} {decoder_state.logprob:12f}: Picked attachment point {attachment_point_pick} - p={attachment_point_logprob:5f}\")\n",
    "                require_bond_states.append(\n",
    "                    MoLeRDecoderState.new_with_focus_on_attachment_point(\n",
    "                        decoder_state,\n",
    "                        attachment_point_pick,\n",
    "                        focus_atom_logprob=attachment_point_logprob,\n",
    "                        attachment_point_choice_info=attachment_point_choice_info,\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        assert not require_attachment_point_states\n",
    "\n",
    "    # Step 3: Pick fresh bonds and populate the next round of decoding steps:\n",
    "    require_bond_states = restrict_to_beam_size_per_mol(require_bond_states, beam_size)\n",
    "    bond_pick_results = self._decoder_pick_new_bond_types(\n",
    "        decoder_states=require_bond_states,\n",
    "        store_generation_traces=store_generation_traces,\n",
    "        sampling_mode=sampling_mode,\n",
    "    )\n",
    "    for (decoder_state, (bond_picks, edge_choice_info)) in zip(\n",
    "        require_bond_states, bond_pick_results\n",
    "    ):\n",
    "        if len(bond_picks) == 0:\n",
    "            # There were no valid options for this bonds, so we treat this as if\n",
    "            # predicting no more bonds with probability 1.0:\n",
    "            # print(I {decoder_state.molecule_id} {decoder_state.logprob:12f}: No more allowed bonds to node {decoder_state.focus_atom}\")\n",
    "            new_decoder_states.append(\n",
    "                MoLeRDecoderState.new_with_focus_marked_as_visited(\n",
    "                    decoder_state,\n",
    "                    focus_node_finished_logprob=0,\n",
    "                    edge_choice_info=edge_choice_info,\n",
    "                )\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        for (bond_pick, bond_pick_logprob) in bond_picks:\n",
    "            # If the decoder says we need no more bonds for the current focus node,\n",
    "            # we mark this and put the decoder state back for the next expansion round:\n",
    "            if bond_pick is None:\n",
    "                # print(I {decoder_state.molecule_id} {decoder_state.logprob:12f}: Finished connecting bonds to node {decoder_state.focus_atom} - p={bond_pick_logprob:5f}\")\n",
    "                new_decoder_states.append(\n",
    "                    MoLeRDecoderState.new_with_focus_marked_as_visited(\n",
    "                        decoder_state,\n",
    "                        focus_node_finished_logprob=bond_pick_logprob,\n",
    "                        edge_choice_info=edge_choice_info,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                (picked_bond_target, picked_bond_type) = bond_pick\n",
    "\n",
    "                # print(I {decoder_state.molecule_id} {decoder_state.logprob:12f}: Adding {decoder_state.focus_atom}-{picked_bond_type}->{picked_bond_target} - p={bond_pick_logprob:5f}\")\n",
    "                new_decoder_states.append(\n",
    "                    MoLeRDecoderState.new_with_added_bond(\n",
    "                        old_state=decoder_state,\n",
    "                        target_atom_idx=int(\n",
    "                            picked_bond_target\n",
    "                        ),  # Go from np.int32 to pyInt\n",
    "                        bond_type_idx=picked_bond_type,\n",
    "                        bond_logprob=bond_pick_logprob,\n",
    "                        edge_choice_info=edge_choice_info,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Everything is done, restrict to the beam width, and go back to the loop start:\n",
    "    decoder_states = restrict_to_beam_size_per_mol(new_decoder_states, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b27a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79af3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
