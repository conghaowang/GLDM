{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e744607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/data/ongh0068/l1000/pyg_output_playground/train/processed_file_paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8a087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/data/ongh0068/l1000/moler_reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f20fae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MolerDataset, MolerData\n",
    "from utils import pprint_pyg_obj\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset = MolerDataset(\n",
    "    root = '/data/ongh0068', \n",
    "    raw_moler_trace_dataset_parent_folder = '/data/ongh0068/l1000/trace_playground',\n",
    "    output_pyg_trace_dataset_parent_folder = '/data/ongh0068/l1000/pyg_output_playground',\n",
    "    split = 'train',\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=False, follow_batch = [\n",
    "    'correct_edge_choices',\n",
    "    'correct_edge_types',\n",
    "    'valid_edge_choices',\n",
    "    'valid_attachment_point_choices',\n",
    "    'correct_attachment_point_choice',\n",
    "    'correct_node_type_choices',\n",
    "    'original_graph_x',\n",
    "    'correct_first_node_type_choices'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e604534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([126, 32])\n",
      "edge_index: torch.Size([2, 338])\n",
      "original_graph_edge_type: torch.Size([1488])\n",
      "original_graph_node_categorical_features: torch.Size([464])\n",
      "focus_node: torch.Size([16])\n",
      "edge_type: torch.Size([338])\n",
      "edge_features: torch.Size([53, 3])\n",
      "correct_edge_choices: torch.Size([53])\n",
      "correct_edge_choices_batch: torch.Size([53])\n",
      "correct_edge_choices_ptr: torch.Size([17])\n",
      "num_correct_edge_choices: torch.Size([16])\n",
      "stop_node_label: torch.Size([16])\n",
      "valid_edge_choices: torch.Size([53, 2])\n",
      "valid_edge_choices_batch: torch.Size([53])\n",
      "valid_edge_choices_ptr: torch.Size([17])\n",
      "valid_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types_batch: torch.Size([9])\n",
      "correct_edge_types_ptr: torch.Size([17])\n",
      "partial_node_categorical_features: torch.Size([126])\n",
      "correct_attachment_point_choice: torch.Size([0])\n",
      "correct_attachment_point_choice_batch: torch.Size([0])\n",
      "correct_attachment_point_choice_ptr: torch.Size([17])\n",
      "correct_node_type_choices: torch.Size([8, 139])\n",
      "correct_node_type_choices_batch: torch.Size([8])\n",
      "correct_node_type_choices_ptr: torch.Size([17])\n",
      "correct_first_node_type_choices: torch.Size([16, 139])\n",
      "correct_first_node_type_choices_batch: torch.Size([16])\n",
      "correct_first_node_type_choices_ptr: torch.Size([17])\n",
      "sa_score: torch.Size([16])\n",
      "clogp: torch.Size([16])\n",
      "mol_weight: torch.Size([16])\n",
      "qed: torch.Size([16])\n",
      "bertz: torch.Size([16])\n",
      "original_graph_edge_index: torch.Size([2, 1488])\n",
      "original_graph_x: torch.Size([464, 32])\n",
      "original_graph_x_batch: torch.Size([464])\n",
      "original_graph_x_ptr: torch.Size([17])\n",
      "valid_attachment_point_choices: torch.Size([0])\n",
      "valid_attachment_point_choices_batch: torch.Size([0])\n",
      "valid_attachment_point_choices_ptr: torch.Size([17])\n",
      "batch: torch.Size([126])\n",
      "ptr: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "def pprint_pyg_obj(batch):\n",
    "    for key in vars(batch)['_store'].keys():\n",
    "        if key.startswith('_'):\n",
    "            continue\n",
    "        print(f'{key}: {batch[key].shape}')\n",
    "for batch in loader:\n",
    "    pprint_pyg_obj(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bcb5286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 461, 462, 463],\n",
       "        [  1,   2,   4,  ..., 461, 462, 463]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.original_graph_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some information out from the dataset:\n",
    "next_node_type_distribution = dataset.metadata.get(\"train_next_node_type_distribution\")\n",
    "atom_type_nums = [\n",
    "    next_node_type_distribution[dataset.node_type_index_to_string[type_idx]]\n",
    "    for type_idx in range(dataset.num_node_types)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8cce787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 16,  16,  16,  16,  16,  16,  16,  16, 129,  16,  16,  16,  16, 129,\n",
       "        129, 129, 129, 129,  16,  16,  16,  16, 129, 129, 129, 129, 129, 129,\n",
       "         16,  16,  16,  16, 129, 129, 129, 129, 129, 129,  16,  16,  16,  16,\n",
       "        129, 129, 129, 129, 129, 129,  16,  16,  16,  16, 129, 129, 129, 129,\n",
       "        129, 129, 131,  16,  16,  16,  16, 129, 129, 129, 129, 129, 129, 131,\n",
       "         16,  16,  16,  16, 129, 129, 129, 129, 129, 129, 131, 129,  16,  16,\n",
       "         16,  16, 129, 129, 129, 129, 129, 129, 131, 129,  16,  16,  16,  16,\n",
       "        129, 129, 129, 129, 129, 129, 131, 129,   0,   0,   0,   0,   0,   0,\n",
       "         16,  16,  16,  16, 129, 129, 129, 129, 129, 129, 131, 129,   0,   0,\n",
       "          0,   0,   0,   0,  16,  16,  16,  16, 129,  16,  16,  16,  16, 129,\n",
       "        129, 129, 129, 129, 129, 131, 129,   0,   0,   0,   0,   0,   0, 129,\n",
       "         16,  16,  16,  16, 129, 129, 129, 129, 129, 129, 131, 129,   0,   0,\n",
       "          0,   0,   0,   0, 129,  16,  16,  16,  16, 129, 129, 129, 129, 129,\n",
       "        129, 131, 129,   0,   0,   0,   0,   0,   0, 129, 130])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.partial_node_categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994b7ed",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "Depending on the representation of the molecular graph (whether bond type is represented as edge type or as edge attributes), we have to change the function signature of the forward method\n",
    "\n",
    "1. Try out both\n",
    "2. tune number of heads\n",
    "3. Look into softmax aggregation (hyperparams) + also implement sigmoidaggregation as a separate aggregation layer\n",
    "4. Set number of heads as a hyperparam\n",
    "5. Add leakyrelu() + layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0ab358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([193, 32])\n",
      "edge_index: torch.Size([2, 370])\n",
      "original_graph_x: torch.Size([464, 32])\n",
      "original_graph_edge_index: torch.Size([2, 1024])\n",
      "original_graph_edge_type: torch.Size([1024])\n",
      "original_graph_node_categorical_features: torch.Size([464])\n",
      "focus_node: torch.Size([16])\n",
      "edge_type: torch.Size([370])\n",
      "edge_features: torch.Size([86, 3])\n",
      "correct_edge_choices: torch.Size([86])\n",
      "correct_edge_choices_batch: torch.Size([86])\n",
      "correct_edge_choices_ptr: torch.Size([17])\n",
      "num_correct_edge_choices: torch.Size([16])\n",
      "stop_node_label: torch.Size([16])\n",
      "valid_edge_choices: torch.Size([86, 2])\n",
      "valid_edge_choices_batch: torch.Size([86])\n",
      "valid_edge_choices_ptr: torch.Size([17])\n",
      "valid_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types_batch: torch.Size([9])\n",
      "correct_edge_types_ptr: torch.Size([17])\n",
      "partial_node_categorical_features: torch.Size([193])\n",
      "correct_attachment_point_choice: torch.Size([0])\n",
      "correct_attachment_point_choice_batch: torch.Size([0])\n",
      "correct_attachment_point_choice_ptr: torch.Size([17])\n",
      "valid_attachment_point_choices: torch.Size([0])\n",
      "valid_attachment_point_choices_batch: torch.Size([0])\n",
      "valid_attachment_point_choices_ptr: torch.Size([17])\n",
      "correct_node_type_choices: torch.Size([1112])\n",
      "correct_node_type_choices_batch: torch.Size([1112])\n",
      "correct_node_type_choices_ptr: torch.Size([17])\n",
      "correct_first_node_type_choices: torch.Size([2224])\n",
      "sa_score: torch.Size([16])\n",
      "clogp: torch.Size([16])\n",
      "mol_weight: torch.Size([16])\n",
      "qed: torch.Size([16])\n",
      "bertz: torch.Size([16])\n",
      "batch: torch.Size([193])\n",
      "ptr: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(dataset.node_type_index_to_string)\n",
    "embedding_size = 64\n",
    "for batch in loader:\n",
    "    pprint_pyg_obj(batch)\n",
    "    break\n",
    "    \n",
    "embed = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "motif_embeddings = embed(batch.partial_node_categorical_features)\n",
    "batch.x = torch.cat((batch.x, motif_embeddings), axis = -1)\n",
    "\n",
    "from torch_geometric.nn import RGATConv\n",
    "resize_layer = RGATConv(\n",
    "in_channels = batch.x.shape[-1], \n",
    "out_channels = 64, \n",
    "num_relations = 3\n",
    ")\n",
    "\n",
    "\n",
    "batch.x = resize_layer(batch.x, batch.edge_index.long(), batch.edge_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46bab97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import RGATConv\n",
    "from torch_geometric.nn import aggr\n",
    "class GraphEncoder(torch.nn.Module):\n",
    "    \"\"\"For constructing graph level embedding during encoder step\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_feature_dim,\n",
    "        node_feature_dim = 64,\n",
    "        num_layers = 12,\n",
    "        use_intermediate_gnn_results = True,\n",
    "    ):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self._first_layer = RGATConv(\n",
    "            in_channels = input_feature_dim, \n",
    "            out_channels = node_feature_dim, \n",
    "            num_relations = 3\n",
    "        )\n",
    "        \n",
    "        self._encoder_layers = torch.nn.ModuleList(\n",
    "            [RGATConv(\n",
    "                in_channels = node_feature_dim, \n",
    "                out_channels = node_feature_dim, \n",
    "                num_relations = 3\n",
    "                ) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self._softmax_aggr = aggr.SoftmaxAggregation(learn=True)\n",
    "        self._use_intermediate_gnn_results = use_intermediate_gnn_results\n",
    "    def forward(self, node_features, edge_index, edge_type, batch_index):\n",
    "        gnn_results = []\n",
    "        gnn_results += [self._first_layer(node_features, edge_index.long(), edge_type)]\n",
    "        print(gnn_results[-1].shape)\n",
    "        for i, layer in enumerate(self._encoder_layers):\n",
    "            gnn_results += [layer(gnn_results[-1], edge_index.long(), edge_type)]\n",
    "        \n",
    "        if self._use_intermediate_gnn_results:\n",
    "            x = torch.cat(gnn_results, axis = -1)\n",
    "            graph_representations = self._softmax_aggr(x, batch_index)\n",
    "        \n",
    "        else:\n",
    "            graph_representations = self._softmax_aggr(gnn_results[-1], batch_index)\n",
    "        node_representations = torch.cat(gnn_results, axis= -1)\n",
    "        return graph_representations, node_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80204491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193, 64])\n"
     ]
    }
   ],
   "source": [
    "model = GraphEncoder(input_feature_dim = batch.x.shape[-1])\n",
    "input_molecule_representations, _ = model(batch.x, batch.edge_index.long(), batch.edge_type, batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817a46f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 832])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_molecule_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05bcced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4114d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAndLogVarMLP(torch.nn.Module):\n",
    "    \"\"\"For constructing graph level embedding during encoder step\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim = 768,\n",
    "        latent_dim = 512,\n",
    "        num_layers = 1,\n",
    "    ):\n",
    "        super(MeanAndLogVarMLP, self).__init__()\n",
    "        self._mean_log_var_mlp = torch.nn.Linear(input_dim, 2*latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._mean_log_var_mlp(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46d4f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_log_var_mlp = MeanAndLogVarMLP(input_dim = input_molecule_representations.shape[-1])\n",
    "x = mean_log_var_mlp(input_molecule_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc95dbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "\n",
    "# this represents the aggregated graph representations after passing through \n",
    "# the mean and log var mlp => we need to sample from this distribution\n",
    "# for each of the partial graphs. There are 16 partial graphs, so \n",
    "# we need to get a separate sample for each of them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a28a1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x[:, : latent_dim]  # Shape: [V, MD]\n",
    "log_var = x[:, latent_dim :]  # Shape: [V, MD]\n",
    "\n",
    "# result_representations: shape [num_partial_graphs, latent_repr_dim]\n",
    "std = torch.exp(log_var / 2)\n",
    "p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "q = torch.distributions.Normal(mu, std)\n",
    "z = q.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24046627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2dac7",
   "metadata": {},
   "source": [
    "# Decoder\n",
    " Every MLP should condition on the latent representation (currently computed from the partial graph itself which is wrong, should be from the original full molecular graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83885293",
   "metadata": {},
   "source": [
    "## PickAtomOrMotif + Node type selection loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de54a3fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GraphEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mencoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PartialGraphEncoder\n\u001b[0;32m----> 4\u001b[0m decoder_gnn \u001b[38;5;241m=\u001b[39m \u001b[43mGraphEncoder\u001b[49m(input_feature_dim \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GraphEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "from encoder import PartialGraphEncoder\n",
    "\n",
    "\n",
    "decoder_gnn = PartialGraphEncoder(input_feature_dim = batch.x.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14578290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193, 64])\n"
     ]
    }
   ],
   "source": [
    "decoder_gnn = GraphEncoder(input_feature_dim = batch.x.shape[-1])\n",
    "partial_graph_representions, node_representations = model(batch.x, batch.edge_index.long(), batch.edge_type, batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97aa07fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 832])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_graph_representions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccbb28c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad9dba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([193, 64])\n",
      "edge_index: torch.Size([2, 370])\n",
      "original_graph_x: torch.Size([464, 32])\n",
      "original_graph_edge_index: torch.Size([2, 1024])\n",
      "original_graph_edge_type: torch.Size([1024])\n",
      "original_graph_node_categorical_features: torch.Size([464])\n",
      "focus_node: torch.Size([16])\n",
      "edge_type: torch.Size([370])\n",
      "edge_features: torch.Size([86, 3])\n",
      "correct_edge_choices: torch.Size([86])\n",
      "correct_edge_choices_batch: torch.Size([86])\n",
      "correct_edge_choices_ptr: torch.Size([17])\n",
      "num_correct_edge_choices: torch.Size([16])\n",
      "stop_node_label: torch.Size([16])\n",
      "valid_edge_choices: torch.Size([86, 2])\n",
      "valid_edge_choices_batch: torch.Size([86])\n",
      "valid_edge_choices_ptr: torch.Size([17])\n",
      "valid_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types_batch: torch.Size([9])\n",
      "correct_edge_types_ptr: torch.Size([17])\n",
      "partial_node_categorical_features: torch.Size([193])\n",
      "correct_attachment_point_choice: torch.Size([0])\n",
      "correct_attachment_point_choice_batch: torch.Size([0])\n",
      "correct_attachment_point_choice_ptr: torch.Size([17])\n",
      "valid_attachment_point_choices: torch.Size([0])\n",
      "valid_attachment_point_choices_batch: torch.Size([0])\n",
      "valid_attachment_point_choices_ptr: torch.Size([17])\n",
      "correct_node_type_choices: torch.Size([1112])\n",
      "correct_node_type_choices_batch: torch.Size([1112])\n",
      "correct_node_type_choices_ptr: torch.Size([17])\n",
      "correct_first_node_type_choices: torch.Size([2224])\n",
      "sa_score: torch.Size([16])\n",
      "clogp: torch.Size([16])\n",
      "mol_weight: torch.Size([16])\n",
      "qed: torch.Size([16])\n",
      "bertz: torch.Size([16])\n",
      "batch: torch.Size([193])\n",
      "ptr: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "pprint_pyg_obj(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffcc22b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  5,  7,  9, 11, 12, 14])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_node_type_choices_batch.unique() # graphs requiring node choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1b31a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  139,  139,  278,  278,  278,  417,  417,  556,  556,  695,  695,\n",
       "         834,  973,  973, 1112, 1112])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_node_type_choices_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17110c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_requiring_node_choices = batch.correct_node_type_choices_batch.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "079c242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx_graphs_requiring_node_choices = batch.correct_node_type_choices_ptr[graphs_requiring_node_choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07e89aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 139, 278, 417, 556, 695, 834, 973])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx_graphs_requiring_node_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60cc903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_multihot_labels = []\n",
    "for i in range(len(batch.correct_node_type_choices_ptr)-1):\n",
    "    start_idx = batch.correct_node_type_choices_ptr[i]\n",
    "    end_idx = batch.correct_node_type_choices_ptr[i+1] \n",
    "    if end_idx - start_idx == 0:\n",
    "        continue\n",
    "    node_selection_labels = batch.correct_node_type_choices[start_idx: end_idx]\n",
    "    node_type_multihot_labels += [node_selection_labels]\n",
    "    \n",
    "node_type_multihot_labels = torch.stack(node_type_multihot_labels, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae6c0758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_type_multihot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42e9b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "class PickAtomOrMotifMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_types, latent_vector_dim, hidden_channels=64):\n",
    "        super(PickAtomOrMotifMLP, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, num_node_types + 1) # add 1 for <END OF GENERATION TOKEN>\n",
    "\n",
    "        \n",
    "    def forward(self, original_and_calculated_graph_representations):\n",
    "        print(original_and_calculated_graph_representations.shape)\n",
    "        x = self.hidden_layer1(original_and_calculated_graph_representations)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c88ca5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_selector = PickAtomOrMotifMLP(\n",
    "    num_node_types = dataset.num_node_types,\n",
    "    latent_vector_dim = z.shape[-1] + partial_graph_representions.shape[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e0e3464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1344])\n",
      "torch.Size([8, 1344])\n"
     ]
    }
   ],
   "source": [
    "# result_representations: shape [PG, MD]\n",
    "# number of partial graphs, molecule representation dimension\n",
    "\n",
    "def pick_node_type(\n",
    "    input_molecule_representations,\n",
    "    graph_representations,\n",
    "    graphs_requiring_node_choices\n",
    "):\n",
    "    \n",
    "    relevant_graph_representations = input_molecule_representations[graphs_requiring_node_choices]\n",
    "    relevant_input_molecule_representations = graph_representations[graphs_requiring_node_choices]\n",
    "    original_and_calculated_graph_representations = torch.cat((relevant_graph_representations,relevant_input_molecule_representations), axis = -1)\n",
    "    print(original_and_calculated_graph_representations.shape)\n",
    "    return node_type_selector(original_and_calculated_graph_representations)\n",
    "\n",
    "output = pick_node_type(z, partial_graph_representions, graphs_requiring_node_choices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3b9d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide_loss(loss, num_choices):\n",
    "    \"\"\"Divide `loss` by `num_choices`, but guard against `num_choices` being 0.\"\"\"\n",
    "    return loss / max(num_choices, 1.0) \n",
    "SMALL_NUMBER, BIG_NUMBER = 1e-7, 1e7\n",
    "def compute_neglogprob_for_multihot_objective(\n",
    "    logprobs,\n",
    "    multihot_labels,\n",
    "    per_decision_num_correct_choices,\n",
    "):\n",
    "    # Normalise by number of correct choices and mask out entries for wrong decisions:\n",
    "    return -(\n",
    "        (logprobs + torch.log(per_decision_num_correct_choices + SMALL_NUMBER))\n",
    "        * multihot_labels\n",
    "        / (per_decision_num_correct_choices + SMALL_NUMBER)\n",
    "    )\n",
    "\n",
    "def compute_node_type_selection_loss(\n",
    "    node_type_logits,\n",
    "    node_type_multihot_labels\n",
    "):\n",
    "    per_node_decision_logprobs = torch.nn.functional.log_softmax(node_type_logits, dim = -1)\n",
    "    # Shape: [NTP, NT + 1]\n",
    "    \n",
    "    # number of correct choices for each of the partial graphs that require node choices\n",
    "    per_node_decision_num_correct_choices = torch.sum(node_type_multihot_labels, keepdim = True, axis = -1)\n",
    "    # Shape [NTP, 1]\n",
    "    \n",
    "    per_correct_node_decision_normalised_neglogprob = compute_neglogprob_for_multihot_objective(\n",
    "        logprobs = per_node_decision_logprobs[:, :-1], # separate out the no node prediction\n",
    "        multihot_labels = node_type_multihot_labels,\n",
    "        per_decision_num_correct_choices = per_node_decision_num_correct_choices,\n",
    "    ) # Shape [NTP, NT]\n",
    "    \n",
    "    no_node_decision_correct =(per_node_decision_num_correct_choices == 0.0).sum()  # Shape [NTP]\n",
    "    per_correct_no_node_decision_neglogprob = -(\n",
    "        per_node_decision_logprobs[:, -1]\n",
    "        * torch.squeeze(no_node_decision_correct).type(torch.FloatTensor)\n",
    "    )  # Shape [NTP]\n",
    "\n",
    "#     if self._node_type_loss_weights is not None:\n",
    "#         per_correct_node_decision_normalised_neglogprob *= self._node_type_loss_weights[:-1]\n",
    "#         per_correct_no_node_decision_neglogprob *= self._node_type_loss_weights[-1]\n",
    "    \n",
    "    # Loss is the sum of the masked (no) node decisions, averaged over number of decisions made:\n",
    "    total_node_type_loss = torch.sum(\n",
    "        per_correct_node_decision_normalised_neglogprob\n",
    "    ) + torch.sum(per_correct_no_node_decision_neglogprob)\n",
    "    node_type_loss = safe_divide_loss(\n",
    "        total_node_type_loss, node_type_multihot_labels.shape[0]\n",
    "    )\n",
    "\n",
    "    return node_type_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14fe295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_selection_loss = compute_node_type_selection_loss(\n",
    "    node_type_logits = output,\n",
    "    node_type_multihot_labels = node_type_multihot_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "528977a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_selection_loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df66735",
   "metadata": {},
   "source": [
    "## Pick edge and edge type + Edge candidate loss\n",
    "\n",
    "1. Find out what stop node refers to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dc9673d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2135581705.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [39], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    # logits for the type of edge if it is picked\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def pick_edge(\n",
    "    input_molecule_representations,\n",
    "    graph_representations,\n",
    "    node_representations,\n",
    "    graph_to_focus_node_map,\n",
    "    node_to_graph_map,\n",
    "    candidate_edge_targets,\n",
    "    candidate_edge_features,\n",
    "    graphs_requiring_node_choices\n",
    "):\n",
    "    # given candidate edges in the partial graphs, \n",
    "    #compute logits for the likelihood of adding candidates as well as \n",
    "    # logits for the type of edge if it is picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_pyg_obj(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba67ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_node_idx_in_batch = batch.ptr[:-1] + batch.focus_node # offset each focus node idx by the starting idx of the molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add41206",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_representations[focus_node_idx_in_batch].shape # extract focus node node level repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_node_representations = node_representations[focus_node_idx_in_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b88500",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_and_focus_node_representations = torch.cat(\n",
    "    (input_molecule_representations, partial_graph_representions, focus_node_representations),\n",
    "    axis = -1\n",
    ")\n",
    "\n",
    "# Explanation: at each step, there is a focus node, which is the node we are \n",
    "# focusing on right now in terms of adding another edge to it. When adding a new\n",
    "# edge, the edge can be between the focus node and a variety of other nodes.\n",
    "# This is likely based on valency, and in reality, it is possible that none of the\n",
    "# edge choices are correct (when that generation step is a node addition step)\n",
    "# and not an edge addition step. Regardless, we still want to consider the candidates\n",
    "#\"target\" refers to the node at the other end of the candidate edge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.valid_edge_choices[:, 1] # the 2nd element in each edge is the target since\n",
    "# the 1st element is the focus node at that step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74749c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_edge_target_node_idx = batch.valid_edge_choices[:, 1] + batch.ptr[batch.valid_edge_choices_batch]  # idx offset for the edge choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_graph_idx_per_edge_candidate = batch.batch[candidate_edge_target_node_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac022b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_and_focus_node_representations = torch.cat(\n",
    "    (input_molecule_representations, partial_graph_representions, focus_node_representations),\n",
    "    axis = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdca4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_and_focus_node_representations_per_edge_candidate = graph_and_focus_node_representations[partial_graph_idx_per_edge_candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidate_target_node_representations = node_representations[candidate_edge_target_node_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_edge_features = batch.edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_truncation = 10\n",
    "candidate_edge_features[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa47148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The zeroth element of edge_features is the graph distance. We need to look that up\n",
    "# in the distance embeddings:\n",
    "truncated_distances = torch.minimum(\n",
    "    candidate_edge_features[:, 0],\n",
    "    torch.ones(len(candidate_edge_features)) * (distance_truncation - 1),\n",
    ")  # shape: [CE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_distances = truncated_distances.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we want to truncate the distance, we should have an embedding layer for it\n",
    "distance_embedding_layer = torch.nn.Embedding(distance_truncation, 1)\n",
    "\n",
    "distance_embedding = distance_embedding_layer(truncated_distances)\n",
    "\n",
    "edge_candidate_representation = torch.cat(\n",
    "    (\n",
    "        graph_and_focus_node_representations_per_edge_candidate, \n",
    "        edge_candidate_target_node_representations,\n",
    "        distance_embedding,\n",
    "        candidate_edge_features[:, 1:],\n",
    "    ),\n",
    "    axis = -1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidate_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956873ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_and_focus_node_representations.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795dccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.nn.Parameter(torch.FloatTensor(1,node_representations.shape[-1] + batch.edge_features.shape[-1]), requires_grad = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "_no_more_edges_representation = torch.nn.Parameter(torch.FloatTensor(1,node_representations.shape[-1] + batch.edge_features.shape[-1]), requires_grad = True)\n",
    "# Calculate the stop node features as well.\n",
    "num_graphs_in_batch= graph_and_focus_node_representations.shape[0]\n",
    "stop_edge_selection_representation = torch.cat(\n",
    "    [\n",
    "        graph_and_focus_node_representations,\n",
    "        torch.tile(\n",
    "            _no_more_edges_representation,\n",
    "            dims=(num_graphs_in_batch, 1),\n",
    "        ),\n",
    "    ],\n",
    "    axis=-1,\n",
    ")  # shape: [PG, MD + PD + 2 * VD*(num_layers+1) + FD]\n",
    "\n",
    "edge_candidate_and_stop_features = torch.cat(\n",
    "    [edge_candidate_representation, stop_edge_selection_representation], axis=0\n",
    ")  # shape: [CE + PG, MD + PD + 2 * VD*(num_layers+1) + FD]\n",
    "\n",
    "\n",
    "from torch.nn import Linear\n",
    "class EdgeCandidateScorer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, latent_vector_dim, output_size = 1, hidden_channels=64):\n",
    "        super(EdgeCandidateScorer, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, output_size) # add 1 for <END OF GENERATION TOKEN>\n",
    "\n",
    "        \n",
    "    def forward(self, original_and_calculated_graph_representations):\n",
    "        x = self.hidden_layer1(original_and_calculated_graph_representations)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return x\n",
    "\n",
    "class EdgeTypeSelector(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, latent_vector_dim,num_edge_types = 3, hidden_channels=64):\n",
    "        super(EdgeTypeSelector, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, num_edge_types) # add 1 for <END OF GENERATION TOKEN>\n",
    "\n",
    "        \n",
    "    def forward(self, original_and_calculated_graph_representations):\n",
    "        x = self.hidden_layer1(original_and_calculated_graph_representations)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return x\n",
    "\n",
    "_edge_candidate_scorer = EdgeCandidateScorer(latent_vector_dim = edge_candidate_and_stop_features.shape[-1])\n",
    "_edge_type_selector = EdgeTypeSelector(latent_vector_dim = edge_candidate_and_stop_features.shape[-1])\n",
    "edge_candidate_logits = torch.squeeze(\n",
    "    _edge_candidate_scorer(edge_candidate_and_stop_features),\n",
    "    axis=-1,\n",
    ")  # shape: [CE + PG]\n",
    "edge_type_logits = _edge_type_selector(\n",
    "    edge_candidate_representation\n",
    ")  # shape: [CE, ET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidate_logits.shape, edge_type_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.correct_edge_choices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d990318",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.correct_edge_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.correct_edge_choices_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidate_to_graph_map = batch.batch[candidate_edge_target_node_idx]\n",
    "# add the end bond labels to the end \n",
    "edge_candidate_to_graph_map = torch.cat((edge_candidate_to_graph_map, torch.arange(0, num_graphs_in_batch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b316435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import scatter \n",
    "def traced_unsorted_segment_log_softmax(\n",
    "    logits, #edge_candidate_logits\n",
    "    segment_ids, #edge_candidate_to_graph_map\n",
    "    num_segments, # num_graphs_in_batch\n",
    "):\n",
    "    \n",
    "    max_per_segment = scatter(logits, segment_ids, reduce = 'max')\n",
    "    scattered_maxes = max_per_segment[segment_ids]\n",
    "    recentered_scores = logits - scattered_maxes\n",
    "    exped_recentered_scores = torch.exp(recentered_scores)\n",
    "\n",
    "    per_segment_sums = scatter(exped_recentered_scores, segment_ids, reduce = 'sum')\n",
    "    per_segment_normalization_consts = torch.log(per_segment_sums)\n",
    "\n",
    "    log_probs = recentered_scores - per_segment_normalization_consts[segment_ids]\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidate_logprobs = traced_unsorted_segment_log_softmax(\n",
    "    logits = edge_candidate_logits,\n",
    "    segment_ids = edge_candidate_to_graph_map,\n",
    "    num_segments = num_graphs_in_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_candidate_selection_loss(\n",
    "    num_graphs_in_batch, # correct_node_type_choices\n",
    "    node_to_graph_map, #batch.batch\n",
    "    candidate_edge_targets, # batch_features[\"valid_edge_choices\"][:, 1]\n",
    "    edge_candidate_logits, # as is\n",
    "    per_graph_num_correct_edge_choices, # batch.num_correct_edge_choices\n",
    "    edge_candidate_correctness_labels, # correct edge choices\n",
    "    no_edge_selected_labels # stop node label\n",
    "):\n",
    "    \n",
    "    # First, we construct full labels for all edge decisions, which are the concat of\n",
    "    # edge candidate logits and the logits for choosing no edge:\n",
    "    edge_correctness_labels = torch.cat(\n",
    "        [edge_candidate_correctness_labels, no_edge_selected_labels.float()],\n",
    "        axis=0,\n",
    "    )  # Shape: [CE + PG]\n",
    "\n",
    "    # To compute a softmax over all candidate edges (and the \"no edge\" choice) corresponding\n",
    "    # to the same graph, we first need to build the map from each logit to the corresponding\n",
    "    # graph id. Then, we can do an unsorted_segment_softmax using that map:\n",
    "    edge_candidate_to_graph_map = batch.batch[candidate_edge_target_node_idx]\n",
    "    # add the end bond labels to the end \n",
    "    edge_candidate_to_graph_map = torch.cat((edge_candidate_to_graph_map, torch.arange(0, num_graphs_in_batch)))\n",
    "\n",
    "    edge_candidate_logprobs = traced_unsorted_segment_log_softmax(\n",
    "        logits=edge_candidate_logits,\n",
    "        segment_ids=edge_candidate_to_graph_map,\n",
    "        num_segments=num_graphs_in_batch,\n",
    "    )  # Shape: [CE + PG]\n",
    "\n",
    "    # Compute the edge loss with the multihot objective.\n",
    "    # For a single graph with three valid choices (+ stop node) of which two are correct,\n",
    "    # we may have the following:\n",
    "    #  edge_candidate_logprobs = log([0.05, 0.5, 0.4, 0.05])\n",
    "    #  per_graph_num_correct_edge_choices = [2]\n",
    "    #  edge_candidate_correctness_labels = [0.0, 1.0, 1.0]\n",
    "    #  edge_correctness_labels = [0.0, 1.0, 1.0, 0.0]\n",
    "    # To get the loss, we simply look at the things in edge_candidate_logprobs that correspond\n",
    "    # to correct entries.\n",
    "    # However, to account for the _multi_hot nature, we scale up each entry of\n",
    "    # edge_candidate_logprobs by the number of correct choices, i.e., consider the\n",
    "    # correct entries of\n",
    "    #  log([0.05, 0.5, 0.4, 0.05]) + log([2, 2, 2, 2]) = log([0.1, 1.0, 0.8, 0.1])\n",
    "    # In this form, we want to have each correct entry to be as near possible to 1.\n",
    "    # Finally, we normalise loss contributions to by-graph, by dividing the crossentropy\n",
    "    # loss by the number of correct choices (i.e., in the example above, this results in\n",
    "    # a loss of -((log(1.0) + log(0.8)) / 2) = 0.11...).\n",
    "\n",
    "    # Note: per_graph_num_correct_edge_choices does not include the choice of an edge to\n",
    "    # the stop node, so can be zero.\n",
    "    per_graph_num_correct_edge_choices = torch.max(\n",
    "        per_graph_num_correct_edge_choices, torch.ones(per_graph_num_correct_edge_choices.shape)\n",
    "    )  # Shape: [PG]\n",
    "\n",
    "\n",
    "    per_edge_candidate_num_correct_choices = per_graph_num_correct_edge_choices[edge_candidate_to_graph_map]\n",
    "    # Shape: [CE]\n",
    "    per_correct_edge_neglogprob = -(\n",
    "        (edge_candidate_logprobs + torch.log(per_edge_candidate_num_correct_choices))\n",
    "        * edge_correctness_labels\n",
    "        / per_edge_candidate_num_correct_choices\n",
    "    )  # Shape: [CE]\n",
    "    \n",
    "    # Normalise by number of graphs for which we made edge selection decisions:\n",
    "    edge_loss = safe_divide_loss(\n",
    "        torch.sum(per_correct_edge_neglogprob), num_graphs_in_batch\n",
    "    )\n",
    "    print(edge_correctness_labels, per_edge_candidate_num_correct_choices)\n",
    "\n",
    "    return edge_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_edge_candidate_selection_loss(\n",
    "    num_graphs_in_batch = len(batch.ptr) -1, # correct_node_type_choices\n",
    "    node_to_graph_map = batch.batch, #batch.batch\n",
    "    candidate_edge_targets = batch.valid_edge_choices[:, 1], # batch_features[\"valid_edge_choices\"][:, 1]\n",
    "    edge_candidate_logits = edge_candidate_logits, # as is\n",
    "    per_graph_num_correct_edge_choices = batch.num_correct_edge_choices, # batch.num_correct_edge_choices\n",
    "    edge_candidate_correctness_labels = batch.correct_edge_choices, # correct edge choices\n",
    "    no_edge_selected_labels = batch.stop_node_label.float() # stop node label\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e60632",
   "metadata": {},
   "source": [
    "## compute_edge_type_selection_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_target_indices = batch.correct_edge_choices != 0\n",
    "edge_type_logits_for_correct_edges = edge_type_logits[correct_target_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fa80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.correct_edge_choices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad540065",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_NUMBER = 1e7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_edge_types = batch.valid_edge_types\n",
    "edge_type_onehot_labels = batch.correct_edge_types\n",
    "\n",
    "\n",
    "# The `valid_edge_types` tensor is equal to 1 when the edge is valid (it may be invalid due\n",
    "# to valency constraints), 0 otherwise.\n",
    "# We want to multiply the selection probabilities by this mask. Because the logits are in\n",
    "# log space, we instead subtract a large value from the logits wherever this mask is zero.\n",
    "scaled_edge_mask = (1-valid_edge_types.float()) * BIG_NUMBER # Shape: [CCE, ET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_edge_type_logits = (\n",
    "    edge_type_logits_for_correct_edges - scaled_edge_mask\n",
    ")  # Shape: [CCE, ET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bcc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cross_entropy(edge_type_onehot_labels.float(), masked_edge_type_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bb44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type_loss= torch.nn.CrossEntropyLoss(reduction = 'none')(edge_type_onehot_labels.float(), masked_edge_type_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70180a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise by the number of edges for which we needed to pick a type:\n",
    "# instead of mean, we must use safe divide because the batch can have zero edges \n",
    "# requring edge types\n",
    "edge_type_loss = safe_divide_loss(\n",
    "    torch.sum(edge_type_loss), len(edge_type_loss)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eaa0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac816e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a89ec3",
   "metadata": {},
   "source": [
    "# pick attachement point + compute pick attachement point loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_attachement_point(\n",
    "    input_molecule_representations, # as is\n",
    "    graph_representations, # partial_graph_representions\n",
    "    node_representations, #as is\n",
    "    node_to_graph_map, # batch.batch\n",
    "    candidate_attachment_points, # \n",
    "):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for batch in loader:\n",
    "    if len(batch.correct_attachment_point_choice) > 0 :\n",
    "        tmp.append(batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a435fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.valid_attachment_point_choices_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.valid_attachment_point_choices_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.valid_attachment_point_choices_ptr.shape, batch.valid_attachment_point_choices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7030c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphEncoder(input_feature_dim = batch.x.shape[-1])\n",
    "input_molecule_representations, _ = model(batch.x, batch.edge_index, batch.edge_type.int(), batch.batch)\n",
    "\n",
    "\n",
    "decoder_gnn = GraphEncoder(input_feature_dim = batch.x.shape[-1])\n",
    "partial_graph_representions, node_representations = model(batch.x, batch.edge_index, batch.edge_type.int(), batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5639fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_and_calculated_graph_representations = torch.cat(\n",
    "    [input_molecule_representations, partial_graph_representions],\n",
    "    axis=-1,\n",
    ")  # Shape: [PG, MD + PD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91429541",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_attachment_pt_corresp_partial_graph_idx = batch.ptr[batch.valid_attachment_point_choices_batch]\n",
    "candidate_attachment_pt_node_idx = (attachment_pt_corresp_partial_graph_idx + batch.valid_attachment_point_choices).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad45cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the partial graphs that have attachment point node candidates.\n",
    "partial_graphs_for_attachment_point_choices = batch.valid_attachment_point_choices_batch\n",
    "\n",
    "# Shape: [CA]\n",
    "\n",
    "\n",
    "# To score an attachment point, we condition on the representations of input and partial\n",
    "# graphs, along with the representation of the attachment point candidate in question.\n",
    "attachment_point_representations = torch.cat(\n",
    "    [\n",
    "        original_and_calculated_graph_representations[partial_graphs_for_attachment_point_choices],\n",
    "        node_representations[attachment_pt_node_idx],\n",
    "    ],\n",
    "    axis=-1,\n",
    ")  # Shape: [CA, MD + PD + VD*(num_layers+1)]\n",
    "\n",
    "\n",
    "\n",
    "class AttachmentPointScorer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, latent_vector_dim, output_size = 1, hidden_channels=64):\n",
    "        super(AttachmentPointScorer, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, output_size) # add 1 for <END OF GENERATION TOKEN>\n",
    "\n",
    "        \n",
    "    def forward(self, original_and_calculated_graph_representations):\n",
    "        x = self.hidden_layer1(original_and_calculated_graph_representations)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_attachment_point_selector = AttachmentPointScorer(latent_vector_dim = attachment_point_representations.shape[-1])\n",
    "attachment_point_selection_logits = torch.squeeze(_attachment_point_selector(attachment_point_representations), axis = -1)\n",
    "# Shape: [CA]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a607213",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachment_point_selection_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attachment_point_selection_loss(\n",
    "    num_graphs_in_batch, # len(batch.ptr)- 1\n",
    "    node_to_graph_map, # batch.batch\n",
    "    attachment_point_selection_logits, # as is\n",
    "    attachment_point_candidate_choices, # valid_attachment_point_choices\n",
    "    attachment_point_correct_choices, # correct_attachment_point_choices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachment_point_candidate_to_graph_map = batch.valid_attachment_point_choices_batch\n",
    "# Shape: [CA]\n",
    "\n",
    "# Compute log softmax of the logits within each partial graph.\n",
    "attachment_point_candidate_logprobs = (\n",
    "    traced_unsorted_segment_log_softmax(\n",
    "        logits=attachment_point_selection_logits,\n",
    "        segment_ids=attachment_point_candidate_to_graph_map,\n",
    "        num_segments=num_graphs_in_batch,\n",
    "    )\n",
    "    * 1.0\n",
    ")  # Shape: [CA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdda1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachment_point_candidate_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace72aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_graph_idx_requiring_attachement_pts, lengths_of_attachement_point_choices_per_graph = batch.valid_attachment_point_choices_batch.unique(return_counts = True)\n",
    "attachement_pt_corrrect_choices_offset = lengths_of_attachement_point_choices_per_graph[:-1]\n",
    "\n",
    "\n",
    "attachment_point_correct_choices = batch.correct_attachment_point_choice\n",
    "# offset the correct choices from the 2nd idx onwards due to batching\n",
    "attachment_point_correct_choices[1:]  = attachement_pt_corrrect_choices_offset\n",
    "\n",
    "attachment_point_correct_choice_neglogprobs = -attachment_point_candidate_logprobs[(attachment_point_correct_choices).long()]\n",
    " # Shape: [AP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dca6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachment_point_correct_choice_neglogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachment_point_selection_loss = safe_divide_loss(\n",
    "    (attachment_point_correct_choice_neglogprobs).sum(),\n",
    "    attachment_point_correct_choice_neglogprobs.shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachment_point_selection_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6c882",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "1. (DONE) Investigate why focus_node, graph properties and attachment points are all empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b33e2",
   "metadata": {},
   "source": [
    "    edge_loss = self.compute_edge_candidate_selection_loss(\n",
    "        num_graphs_in_batch=batch_features[\"num_partial_graphs_in_batch\"],\n",
    "        node_to_graph_map=batch_features[\"node_to_partial_graph_map\"],\n",
    "        candidate_edge_targets=batch_features[\"valid_edge_choices\"][:, 1],\n",
    "        edge_candidate_logits=task_output.edge_candidate_logits,\n",
    "        per_graph_num_correct_edge_choices=batch_labels[\"num_correct_edge_choices\"],\n",
    "        edge_candidate_correctness_labels=batch_labels[\"correct_edge_choices\"],\n",
    "        no_edge_selected_labels=batch_labels[\"stop_node_label\"],\n",
    "    )\n",
    "\n",
    "Shape abbreviations used throughout the model:\n",
    "- PG = number of _p_artial _g_raphs\n",
    "- PV = number of _p_artial graph _v_ertices\n",
    "- PD = size of _p_artial graph _representation _d_imension\n",
    "- VD = GNN _v_ertex representation _d_imension\n",
    "- MD = _m_olecule representation _d_imension\n",
    "- EFD = _e_dge _f_eature _d_imension\n",
    "- NTP = number of partial graphs requiring a _n_ode _t_ype _p_ick\n",
    "- NT = number of _n_ode _t_ypes\n",
    "- CE = number of _c_andidate _e_dges\n",
    "- CCE = number of _c_orrect _c_andidate _e_dges\n",
    "- ET = number of _e_dge _t_ypes\n",
    "- CA = number of _c_andidate _a_ttachment points\n",
    "- AP = number of _a_ttachment _p_oint choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99eabe",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Implement Sampling strategy\n",
    "2. Look into why first node prediction is treated separately\n",
    "3. Add dropout layers\n",
    "4. investigate why the losses are all so huge\n",
    "\n",
    "Current design plan: all model specific things are in the BaseModel class, \n",
    "All loss computation and what not will be in the lightning module \n",
    "This decouples the training code, optimizers and schedulers from the model itself, which arguably makes it cleaner when doing hyperparameter tuning and also during model loading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note: Original implementation of the GNN adds each molecule and their corresponding generation steps sequentially into the batch, so what happens is that a molecule's sequential generation steps are seen by the GNN during training in a sequential manner. In this implementation, if we use shuffle = False, we can replicate this behaviour in our dataloader, but if we use shuffle = True, then we can't. \n",
    "\n",
    "Possible additions to allow for molecule wise shuffling => add additional column to CSV to allow for a molecule's generation steps to be treated as one set of training samples => then shuffle this instead of the actual generation steps (try this if the training is not converging)\n",
    "\n",
    "This has additional implications for the latent space sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MoLeROutput:\n",
    "    \"\"\"Class for keeping track of output from MoLeR model.\"\"\"\n",
    "    node_type_logits\n",
    "    edge_candidate_logits\n",
    "    edge_type_logits\n",
    "    attachment_point_selection_logits\n",
    "    p\n",
    "    q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e010f0",
   "metadata": {},
   "source": [
    "# Follow implementation here: https://github.com/Lightning-AI/lightning-bolts/blob/master/pl_bolts/models/autoencoders/basic_vae/basic_vae_module.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438dc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from molecule_generation.utils.training_utils import get_class_balancing_weights\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "\n",
    "\n",
    "class BaseModel(LightningModule):\n",
    "    def __init__(self, params, dataset):\n",
    "        \"\"\"Params is a nested dictionary with the relevant parameters.\"\"\"\n",
    "        super(BaseModel, self).__init__()\n",
    "        self._init_params(params, dataset)\n",
    "        self._motif_aware_embedding_layer = Embedding(params['motif_aware_embedding'])\n",
    "        self._encoder_gnn = FullGraphEncoder(params['encoder_gnn'])\n",
    "        self._decoder_gnn = PartialGraphEncoder(params['decoder_gnn'])\n",
    "        self._decoder_mlp = MLPDecoder(params['decoder_mlp'])\n",
    "        \n",
    "        # params for latent space\n",
    "        self._latent_sample_strategy = params['latent_sample_strategy']\n",
    "        self._latent_repr_dim = params[\"latent_repr_size\"]\n",
    "        \n",
    "        \n",
    "               \n",
    "    def _init_params(self, params, dataset):\n",
    "        \"\"\"\n",
    "        Initialise class weights for next node prediction and placefolder for\n",
    "        motif/node embeddings.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get some information out from the dataset:\n",
    "        next_node_type_distribution = dataset.metadata.get(\"train_next_node_type_distribution\")\n",
    "        class_weight_factor = self._params.get(\"node_type_predictor_class_loss_weight_factor\", 0.0)\n",
    "        \n",
    "        if not (0.0 <= class_weight_factor <= 1.0):\n",
    "            raise ValueError(\n",
    "                f\"Node class loss weight node_classifier_class_loss_weight_factor must be in [0,1], but is {class_weight_factor}!\"\n",
    "            )\n",
    "        if class_weight_factor > 0:\n",
    "            atom_type_nums = [\n",
    "                next_node_type_distribution[dataset.node_type_index_to_string[type_idx]]\n",
    "                for type_idx in range(dataset.num_node_types)\n",
    "            ]\n",
    "            atom_type_nums.append(next_node_type_distribution[\"None\"])\n",
    "\n",
    "            self.class_weights = get_class_balancing_weights(\n",
    "                class_counts=atom_type_nums, class_weight_factor=class_weight_factor\n",
    "            )\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "            \n",
    "        motif_vocabulary = dataset.metadata.get(\"motif_vocabulary\")\n",
    "        self._uses_motifs = motif_vocabulary is not None\n",
    "\n",
    "        self._node_categorical_num_classes = dataset.node_categorical_num_classes\n",
    "        \n",
    "        \n",
    "        if self.uses_categorical_features:\n",
    "            if \"categorical_features_embedding_dim\" in self._params:\n",
    "                self._node_categorical_features_embedding = None\n",
    "        \n",
    "    @property\n",
    "    def uses_motifs(self):\n",
    "        return self._uses_motifs\n",
    "\n",
    "    @property\n",
    "    def uses_categorical_features(self):\n",
    "        return self._node_categorical_num_classes is not None\n",
    "\n",
    "    @property\n",
    "    def decoder(self):\n",
    "        return self._decoder\n",
    "\n",
    "    @property\n",
    "    def encoder(self):\n",
    "        return self._encoder\n",
    "    \n",
    "    @property\n",
    "    def motif_aware_embedding_layer(self):\n",
    "        return self._motif_aware_embedding_layer\n",
    "    \n",
    "    @property\n",
    "    def latent_dim(self):\n",
    "        return self._latent_repr_dim\n",
    "    \n",
    "    def compute_initial_node_features(batch )\n",
    "        # Compute embedding\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def sample_from_latent_repr(latent_repr):\n",
    "        # perturb latent repr\n",
    "        mu = latent_repr[:, : self.latent_dim]  # Shape: [V, MD]\n",
    "        log_var = latent_repr[:, self.latent_dim :]  # Shape: [V, MD]\n",
    "\n",
    "        # result_representations: shape [num_partial_graphs, latent_repr_dim]\n",
    "        p, q, z = self.sample(mu, log_var)\n",
    "        \n",
    "        return p, q, z \n",
    "        \n",
    "    def sample(self, mu, log_var)\n",
    "        \"\"\"Samples a different noise vector for each partial graph. \n",
    "        TODO: look into the other sampling strategies.\"\"\"\n",
    "        std = torch.exp(log_var / 2)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "        return p, q, z\n",
    "    \n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch, ??):\n",
    "        # Obtain node embeddings \n",
    "        batch = self.compute_initial_node_features(batch)\n",
    "        \n",
    "        # Forward pass through encoder\n",
    "        latent_repr = self.encoder(batch)\n",
    "        \n",
    "        # Apply latent sampling strategy\n",
    "        p, q, latent_repr = self.sample_from_latent_repr(latent_repr)\n",
    "        \n",
    "        # Forward pass through decoder\n",
    "        node_type_logits, edge_candidate_logits, edge_type_logits, attachment_point_selection_logits = self.decoder(latent_repr)\n",
    "        \n",
    "        # NOTE: loss computation will be done in lightning module\n",
    "        return MoLeROutput(\n",
    "            node_type_logits = node_type_logits,\n",
    "            edge_candidate_logits = edge_candidate_logits,\n",
    "            edge_type_logits = edge_type_logits,\n",
    "            attachment_point_selection_logits = attachment_point_selection_logits,\n",
    "            p = p,\n",
    "            q = q,\n",
    "        )\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "#         x = self.conv1(x, edge_index, edge_attr)\n",
    "#         x = x.relu()\n",
    "#         x = self.conv2(x, edge_index, edge_attr)\n",
    "#         x = x.relu()\n",
    "#         x = self.conv3(x, edge_index, edge_attr)\n",
    "#         return x\n",
    "#     def _compute_decoder_loss_and_metrics(\n",
    "#         self, batch_features, task_output, batch_labels\n",
    "#     ) -> Tuple[tf.Tensor, MoLeRDecoderMetrics]:\n",
    "\n",
    "#         decoder_metrics = self.decoder.compute_metrics(\n",
    "#             batch_features=batch_features, batch_labels=batch_labels, task_output=task_output\n",
    "#         )\n",
    "\n",
    "#         total_loss = (\n",
    "#             self._params[\"node_classification_loss_weight\"]\n",
    "#             * decoder_metrics.node_classification_loss\n",
    "#             + self._params[\"first_node_classification_loss_weight\"]\n",
    "#             * decoder_metrics.first_node_classification_loss\n",
    "#             + self._params[\"edge_selection_loss_weight\"] * decoder_metrics.edge_loss\n",
    "#             + self._params[\"edge_type_loss_weight\"] * decoder_metrics.edge_type_loss\n",
    "#         )\n",
    "\n",
    "#         if self.uses_motifs:\n",
    "#             total_loss += (\n",
    "#                 self._params[\"attachment_point_selection_weight\"]\n",
    "#                 * decoder_metrics.attachment_point_selection_loss\n",
    "#             )\n",
    "\n",
    "#         return total_loss, decoder_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd147f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4c98cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(torch.nn.Module):\n",
    "    \"\"\"For constructing graph level embedding during encoder step\"\"\"\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        # torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.leaky_relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.leaky_relu()\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = \n",
    "        return x\n",
    "\n",
    "\n",
    "class PartialGraphEncoder(torch.nn.Module):\n",
    "    \"\"\"For constructing graph level embedding during decoder steps\"\"\"\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(PartialGraphEncoder, self).__init__()\n",
    "        # torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        return x\n",
    "  \n",
    "\n",
    "class PickAtomOrMotifMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, motif_vocabulary, latent_vector_dim, hidden_channels=256):\n",
    "        super(PickAtomOrMotifMLP, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, len(motif_vocabulary) + 1) # add 1 for <END OF GENERATION TOKEN>\n",
    "        self.activation = torch.Softmax\n",
    "        \n",
    "    def forward(self, latent_vector, partial_graph_embedding):\n",
    "        x = self.hidden_layer1(latent_vector)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return self.activation(x)\n",
    "        \n",
    "\n",
    "class PickAttachmentMLP(torch.nn.Module):\n",
    "    def __init__(self, motif_dictionary, hidden_channels):\n",
    "        super(PickAttachmentMLP, self).__init__()\n",
    "        pass\n",
    "    def forward(self, latent_vector, partial_graph_embedding):\n",
    "        pass\n",
    "\n",
    "class PickBond(torch.nn.Module):\n",
    "    def __init__(self, motif_dictionary, hidden_channels):\n",
    "        super(PickAttachmentMLP, self).__init__()\n",
    "        pass\n",
    "    def forward(self, latent_vector, partial_graph_embedding):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776f10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5585c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849029d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30276ddd",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "1. Implement the learned aggregation function in Moler Paper by subclassing MessagePassing layer \n",
    "2. Change the init function of the encoder to take in the relevant hyperparameters from the `moler_vae` file\n",
    "3. Figure out which of the latent sample strategies work best:\n",
    "- pass through\n",
    "- per graph\n",
    "- per partial graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f28cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
