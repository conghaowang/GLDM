{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bdca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.utils import softmax\n",
    "from model_utils import GenericMLP\n",
    "from utils import unsorted_segment_softmax\n",
    "import torch\n",
    "class WeightedSumGraphRepresentation(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_feature_dim,\n",
    "        graph_representation_size,\n",
    "        num_heads,\n",
    "        weighting_fun = \"softmax\",  # One of {\"softmax\", \"sigmoid\"}\n",
    "        scoring_mlp_layers = [128],\n",
    "        scoring_mlp_activation_fun = \"ReLU\",\n",
    "        scoring_mlp_use_biases: bool = False,\n",
    "        scoring_mlp_dropout_rate = 0.1,\n",
    "        transformation_mlp_layers = [128],\n",
    "        transformation_mlp_activation_fun = \"ReLU\",\n",
    "        transformation_mlp_use_biases = False,\n",
    "        transformation_mlp_dropout_rate = 0.2,\n",
    "#         transformation_mlp_result_lower_bound = None,\n",
    "#         transformation_mlp_result_upper_bound = None,\n",
    "#         **kwargs,\n",
    "    ):\n",
    "        self._num_heads = num_heads\n",
    "        self._graph_representation_size = graph_representation_size\n",
    "        self._weighting_fun = weighting_fun.lower()\n",
    "        assert self._weighting_fun in ['softmax', 'sigmoid']\n",
    "        self._transformation_mlp_activation_fun = ReLU()\n",
    "        self._scoring_mlp = GenericMLP(\n",
    "            input_feature_dim = input_feature_dim,\n",
    "            output_size = self._num_heads, # one score for each head\n",
    "            hidden_layer_feature_dim=256,\n",
    "            num_hidden_layers=1,\n",
    "            activation_layer_type=\"leaky_relu\",\n",
    "            dropout_prob=scoring_mlp_dropout_rate,\n",
    "        )\n",
    "        self._transformation_mlp = GenericMLP(\n",
    "            input_feature_dim = input_feature_dim,\n",
    "            output_size = self._graph_representation_size, # one score for each head\n",
    "            hidden_layer_feature_dim=256,\n",
    "            num_hidden_layers=1,\n",
    "            activation_layer_type=\"leaky_relu\",\n",
    "            dropout_prob=transformation_mlp_dropout_rate,\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x, \n",
    "        batch = None, \n",
    "    ):\n",
    "        # (1) compute weights for each node/head pair:\n",
    "        scores = self_scoring_mlp(x) # Shape [number of nodes, number of heads]\n",
    "        if self._weighting_fun == 'sigmoid':\n",
    "            weights = torch.sigmoid(scores)\n",
    "        elif self._weighting_fun == 'softmax':\n",
    "            weights_per_head = []\n",
    "            for head_idx in range(self._num_heads):\n",
    "                head_scores = scores[:, head_idx] # Shape [V]\n",
    "                head_weights = unsorted_segment_softmax(\n",
    "                    logits = head_scores,\n",
    "                    segment_ids = batch\n",
    "                ) # Shape [V]\n",
    "                weights_per_head.append(head_weights)\n",
    "            weights = torch.cat(weights_per_head, axis = -1) # Shape [V, H]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        # (2) compute representations for each node/head pair:\n",
    "        node_reprs = self._transformation_mlp_activation_fun(self._transformation_mlp(x))\n",
    "        # Shape [V, graph representation dimension]\n",
    "        \n",
    "        node_reprs = node_reprs.view(-1, self.num_heads, self._graph_representation_size//self._num_heads)\n",
    "            \n",
    "        # (3) if necessary, weight representations and aggregate by graph:\n",
    "        weights = torch.unsqueeze(weights, -1)  # Shape [V, H, 1]\n",
    "        weighted_node_reprs = weights * node_reprs  # Shape [V, H, GD//H]\n",
    "\n",
    "        weighted_node_reprs = weighted_node_reprs.view(-1, self._graph_representation_size)\n",
    "        # Shape [V, GD]\n",
    "        graph_reprs = scatter(weighted_node_reprs, batch, reduce = 'sum')  # Shape [G, GD]\n",
    "        return graph_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e014f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 14:42:30.509204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model import BaseModel\n",
    "from dataset import MolerDataset, MolerData\n",
    "from utils import pprint_pyg_obj\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "from model_utils import get_params\n",
    "\n",
    "\n",
    "\n",
    "dataset = MolerDataset(\n",
    "    root = '/data/ongh0068', \n",
    "    raw_moler_trace_dataset_parent_folder = '/data/ongh0068/guacamol/trace_dir',\n",
    "    output_pyg_trace_dataset_parent_folder = '/data/ongh0068/l1000/already_batched',\n",
    "    split = 'valid_0',\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, follow_batch = [\n",
    "    'correct_edge_choices',\n",
    "    'correct_edge_types',\n",
    "    'valid_edge_choices',\n",
    "    'valid_attachment_point_choices',\n",
    "    'correct_attachment_point_choice',\n",
    "    'correct_node_type_choices',\n",
    "    'original_graph_x',\n",
    "    'correct_first_node_type_choices'\n",
    "])\n",
    "\n",
    "\n",
    "for batch in loader:\n",
    "#     batch.cuda()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455f12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import AggrLayerType\n",
    "\n",
    "params = get_params(dataset)\n",
    "\n",
    "params['full_graph_encoder']['aggr_layer_type'] = 'MoLeRAggregation'\n",
    "params['full_graph_encoder']['total_num_moler_aggr_heads'] = 32\n",
    "\n",
    "params['partial_graph_encoder']['aggr_layer_type'] = 'MoLeRAggregation'\n",
    "params['partial_graph_encoder']['total_num_moler_aggr_heads'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6a4e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_graph_encoder': {'input_feature_dim': 59,\n",
       "  'atom_or_motif_vocab_size': 166,\n",
       "  'aggr_layer_type': 'MoLeRAggregation',\n",
       "  'total_num_moler_aggr_heads': 32},\n",
       " 'partial_graph_encoder': {'input_feature_dim': 59,\n",
       "  'atom_or_motif_vocab_size': 166,\n",
       "  'aggr_layer_type': 'MoLeRAggregation',\n",
       "  'total_num_moler_aggr_heads': 16},\n",
       " 'mean_log_var_mlp': {'input_feature_dim': 832, 'output_size': 1024},\n",
       " 'decoder': {'node_type_selector': {'input_feature_dim': 1344,\n",
       "   'output_size': 167},\n",
       "  'node_type_loss_weights': tensor([10.0000,  0.1000,  3.6015,  0.1000,  0.1000,  0.4439,  0.7549,  0.4416,\n",
       "          10.0000,  2.7939,  3.3916, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,  0.1731]),\n",
       "  'no_more_edges_repr': (1, 835),\n",
       "  'edge_candidate_scorer': {'input_feature_dim': 3011, 'output_size': 1},\n",
       "  'edge_type_selector': {'input_feature_dim': 3011, 'output_size': 3},\n",
       "  'attachment_point_selector': {'input_feature_dim': 2176, 'output_size': 1},\n",
       "  'first_node_type_selector': {'input_feature_dim': 512, 'output_size': 166}},\n",
       " 'latent_sample_strategy': 'per_graph',\n",
       " 'latent_repr_dim': 512,\n",
       " 'latent_repr_size': 512,\n",
       " 'kl_divergence_weight': 0.02,\n",
       " 'kl_divergence_annealing_beta': 0.999,\n",
       " 'training_hyperparams': {'max_lr': 0.001,\n",
       "  'div_factor': 25,\n",
       "  'three_phase': True}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f059f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(params, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ca462d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolerDataBatch(x=[15852, 59], edge_index=[2, 31368], original_graph_edge_features=[61982], original_graph_node_categorical_features=[28616], focus_node=[1000], partial_graph_edge_features=[31368], edge_features=[7104, 3], correct_edge_choices=[7104], correct_edge_choices_batch=[7104], correct_edge_choices_ptr=[1001], num_correct_edge_choices=[1000], stop_node_label=[1000], valid_edge_choices=[7104, 2], valid_edge_choices_batch=[7104], valid_edge_choices_ptr=[1001], valid_edge_types=[510, 3], correct_edge_types=[510, 3], correct_edge_types_batch=[510], correct_edge_types_ptr=[1001], partial_node_categorical_features=[15852], correct_attachment_point_choice=[42], correct_attachment_point_choice_batch=[42], correct_attachment_point_choice_ptr=[1001], correct_node_type_choices=[482, 166], correct_node_type_choices_batch=[482], correct_node_type_choices_ptr=[1001], correct_first_node_type_choices=[1000, 166], correct_first_node_type_choices_batch=[1000], correct_first_node_type_choices_ptr=[1001], sa_score=[1000], clogp=[1000], mol_weight=[1000], qed=[1000], bertz=[1000], original_graph_edge_index=[2, 61982], original_graph_x=[28616, 59], original_graph_x_batch=[28616], original_graph_x_ptr=[1001], valid_attachment_point_choices=[168], valid_attachment_point_choices_batch=[168], valid_attachment_point_choices_ptr=[1001], batch=[15852], ptr=[1001])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5369a3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([28616, 16])\n",
      "torch.Size([28616, 16, 1]) torch.Size([28616, 16, 26])\n",
      "torch.Size([28616, 16])\n",
      "torch.Size([28616, 16, 1]) torch.Size([28616, 16, 26])\n",
      "True\n",
      "torch.Size([15852, 8])\n",
      "torch.Size([15852, 8, 1]) torch.Size([15852, 8, 52])\n",
      "torch.Size([15852, 8])\n",
      "torch.Size([15852, 8, 1]) torch.Size([15852, 8, 52])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MoLeROutput(first_node_type_logits=tensor([[ 0.2009, -0.1930,  0.1892,  ..., -0.1715,  0.0229, -0.1759],\n",
       "        [ 0.0147, -0.1725,  0.0655,  ..., -0.1513, -0.0655, -0.2060],\n",
       "        [ 0.0849, -0.1444,  0.1801,  ..., -0.2022, -0.0208, -0.0594],\n",
       "        ...,\n",
       "        [-0.1937, -0.3173,  0.0169,  ..., -0.2271, -0.0100, -0.2113],\n",
       "        [-0.0700, -0.1213,  0.0962,  ..., -0.1686, -0.0179, -0.1278],\n",
       "        [ 0.0282, -0.2687,  0.0998,  ..., -0.0415, -0.1082, -0.0653]],\n",
       "       grad_fn=<AddmmBackward0>), node_type_logits=tensor([[ 0.0409, -0.1150,  0.1528,  ..., -0.0375, -0.0429,  0.1271],\n",
       "        [-0.0032, -0.2015,  0.0424,  ..., -0.0727, -0.0866,  0.0798],\n",
       "        [-0.0857, -0.0029,  0.0542,  ..., -0.0162, -0.0931,  0.0746],\n",
       "        ...,\n",
       "        [-0.1051, -0.1175,  0.1213,  ...,  0.1465, -0.0736,  0.0625],\n",
       "        [-0.0662, -0.1139,  0.0762,  ...,  0.0717, -0.0148,  0.0720],\n",
       "        [ 0.0478, -0.0242,  0.0718,  ...,  0.0753, -0.0562,  0.0475]],\n",
       "       grad_fn=<AddmmBackward0>), edge_candidate_logits=tensor([ 0.0171, -0.0425, -0.0385,  ..., -0.0798, -0.1196, -0.0470],\n",
       "       grad_fn=<SqueezeBackward1>), edge_type_logits=tensor([[ 0.1903,  0.0743, -0.0367],\n",
       "        [ 0.0694,  0.0599, -0.0139],\n",
       "        [ 0.1341, -0.0319, -0.0375],\n",
       "        ...,\n",
       "        [ 0.1531,  0.0147, -0.0162],\n",
       "        [ 0.0953, -0.0223, -0.0061],\n",
       "        [ 0.1147, -0.0034, -0.0290]], grad_fn=<AddmmBackward0>), attachment_point_selection_logits=tensor([-1.6448e-01, -9.8826e-02, -1.7366e-01, -1.4052e-01, -5.0772e-02,\n",
       "        -5.5077e-02, -8.6965e-02, -2.0567e-02,  7.8183e-02,  5.0754e-02,\n",
       "        -3.9067e-03,  3.7098e-02,  1.2912e-02, -8.9193e-04, -3.7533e-02,\n",
       "        -7.2243e-02, -8.9604e-02, -8.4504e-02, -3.3492e-02, -5.5446e-02,\n",
       "        -1.0737e-01, -1.7551e-02,  7.7676e-03, -4.9187e-03,  1.6209e+00,\n",
       "         3.5476e-01, -8.8751e-03, -8.8719e-02, -9.5653e-02, -6.4858e-02,\n",
       "        -1.4897e-01,  1.0452e-01, -1.3085e-02,  2.2750e-01,  1.9418e-01,\n",
       "         1.1532e-01,  2.3861e-01,  1.6373e-01,  1.3687e-01,  4.0554e-02,\n",
       "        -4.4744e-02, -1.5234e-02, -2.3803e-02, -1.5212e-03, -2.2511e-02,\n",
       "        -1.3616e-02,  4.9444e-02, -1.1306e-01, -6.6010e-02, -1.1681e-01,\n",
       "        -1.3794e-02, -8.9299e-02, -2.8079e-02, -8.4377e-02, -5.5003e-02,\n",
       "         6.7278e-03, -5.6484e-02, -3.0713e-02, -2.5845e-02, -4.1527e-02,\n",
       "         3.3699e-02, -5.8699e-02, -3.1113e-02, -9.1722e-02, -7.9706e-02,\n",
       "        -1.0852e-01, -6.4109e-02,  2.3533e-02, -6.0202e-03,  2.7566e-02,\n",
       "        -6.1782e-04, -6.2227e-02, -1.4573e-01,  1.7038e-02, -6.0909e-02,\n",
       "        -3.9428e-02, -8.2544e-02, -6.5530e-02, -7.4801e-02,  1.3114e-02,\n",
       "         3.4155e-04, -1.8548e-03,  3.5892e-02,  1.4359e-02,  7.3107e-03,\n",
       "        -7.7045e-03, -1.4573e-02, -3.6837e-02, -3.4677e-02, -5.6465e-03,\n",
       "        -5.8236e-02,  1.9265e-02,  3.5496e-02,  1.7473e-03,  5.9009e-02,\n",
       "        -1.0793e-02, -6.6076e-03, -1.0733e-02, -4.5501e-02,  1.3552e-02,\n",
       "        -3.7118e-02, -4.2348e-02,  8.5878e-03, -6.3257e-02, -1.0242e-02,\n",
       "        -1.3494e-02, -6.8859e-02, -2.9868e-02, -5.2705e-02, -3.9602e-02,\n",
       "        -4.7127e-02, -7.4027e-02, -3.4904e-02, -1.0962e-01, -8.2208e-02,\n",
       "        -8.1437e-03,  3.8745e-02,  5.3169e-02,  2.9852e-02,  4.4541e-02,\n",
       "         7.3119e-02,  3.5845e-02,  3.1326e-02,  5.3362e-02,  3.2714e-02,\n",
       "        -1.5650e-02,  1.0389e-02,  5.6910e-03, -9.2184e-03,  3.6806e-02,\n",
       "         2.6350e-02, -8.2650e-02, -2.5130e-02, -1.5580e-03, -1.4974e-02,\n",
       "        -7.6829e-03, -3.6727e-02, -1.8459e-02, -1.2265e-01, -8.9019e-02,\n",
       "        -6.8891e-02, -8.8970e-02, -9.6812e-02, -1.1914e-01, -8.4501e-02,\n",
       "        -8.0019e-02,  1.0860e-02, -1.0017e-02,  3.9863e-02,  2.3342e-02,\n",
       "         2.8618e-02,  2.7956e-02,  3.4488e-03, -1.1677e-02, -2.7261e-02,\n",
       "         7.4834e-03, -5.5549e-03,  2.8284e-02, -3.7779e-02,  5.1072e-02,\n",
       "         3.3796e-02,  6.7715e-02,  1.4290e-02,  8.3520e-02,  8.3894e-03,\n",
       "         1.1022e-02,  2.9857e-02,  2.6574e-02], grad_fn=<SqueezeBackward1>), p=Normal(loc: torch.Size([1000, 512]), scale: torch.Size([1000, 512])), q=Normal(loc: torch.Size([1000, 512]), scale: torch.Size([1000, 512])))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._run_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b616efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (_full_graph_encoder): GraphEncoder(\n",
       "    (_embed): Embedding(166, 64)\n",
       "    (_model): GenericGraphEncoder(\n",
       "      (_first_layer): FiLMConv(123, 64, num_relations=3)\n",
       "      (_encoder_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (8): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (9): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (10): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (11): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "      )\n",
       "      (_softmax_aggr): WeightedSumGraphRepresentation(\n",
       "        (_transformation_mlp_activation_fun): LeakyReLU(negative_slope=0.01)\n",
       "        (_scoring_mlp): GenericMLP(\n",
       "          (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "          (_hidden_layers): ModuleList(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (_output_layer): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (_transformation_mlp): GenericMLP(\n",
       "          (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "          (_hidden_layers): ModuleList(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (_output_layer): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=416, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_sigmoid_aggr): WeightedSumGraphRepresentation(\n",
       "        (_transformation_mlp_activation_fun): LeakyReLU(negative_slope=0.01)\n",
       "        (_scoring_mlp): GenericMLP(\n",
       "          (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "          (_hidden_layers): ModuleList(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (_output_layer): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (_transformation_mlp): GenericMLP(\n",
       "          (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "          (_hidden_layers): ModuleList(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (_output_layer): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=416, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_partial_graph_encoder): PartialGraphEncoder(\n",
       "    (_embed): Embedding(166, 64)\n",
       "    (_model): GenericGraphEncoder(\n",
       "      (_first_layer): FiLMConv(124, 64, num_relations=3)\n",
       "      (_encoder_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (8): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (9): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (10): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "        (11): Sequential(\n",
       "          (0): LayerNorm(64, mode=graph)\n",
       "          (1): FiLMConv(64, 64, num_relations=3)\n",
       "        )\n",
       "      )\n",
       "      (_softmax_aggr): WeightedSumGraphRepresentation(\n",
       "        (_transformation_mlp_activation_fun): LeakyReLU(negative_slope=0.01)\n",
       "        (_scoring_mlp): GenericMLP(\n",
       "          (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "          (_hidden_layers): ModuleList(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (_output_layer): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=8, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (_transformation_mlp): GenericMLP(\n",
       "          (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "          (_hidden_layers): ModuleList(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (_output_layer): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=416, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_sigmoid_aggr): WeightedSumGraphRepresentation(\n",
       "        (_transformation_mlp_activation_fun): LeakyReLU(negative_slope=0.01)\n",
       "        (_scoring_mlp): GenericMLP(\n",
       "          (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "          (_hidden_layers): ModuleList(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (_output_layer): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=8, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (_transformation_mlp): GenericMLP(\n",
       "          (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "          (_hidden_layers): ModuleList(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (_output_layer): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.01)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): Linear(in_features=256, out_features=416, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_mean_log_var_mlp): GenericMLP(\n",
       "    (_first_layer): Linear(in_features=832, out_features=256, bias=True)\n",
       "    (_hidden_layers): ModuleList(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (_output_layer): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_decoder): MLPDecoder(\n",
       "    (_first_node_type_selector): GenericMLP(\n",
       "      (_first_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (_hidden_layers): ModuleList(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (_output_layer): Sequential(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=166, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_node_type_selector): GenericMLP(\n",
       "      (_first_layer): Linear(in_features=1344, out_features=256, bias=True)\n",
       "      (_hidden_layers): ModuleList(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (_output_layer): Sequential(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=167, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_edge_candidate_scorer): GenericMLP(\n",
       "      (_first_layer): Linear(in_features=3011, out_features=256, bias=True)\n",
       "      (_hidden_layers): ModuleList(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (_output_layer): Sequential(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_edge_type_selector): GenericMLP(\n",
       "      (_first_layer): Linear(in_features=3011, out_features=256, bias=True)\n",
       "      (_hidden_layers): ModuleList(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (_output_layer): Sequential(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_cross_entropy_loss): CrossEntropyLoss()\n",
       "    (_distance_embedding_layer): Embedding(10, 1)\n",
       "    (_attachment_point_selector): GenericMLP(\n",
       "      (_first_layer): Linear(in_features=2176, out_features=256, bias=True)\n",
       "      (_hidden_layers): ModuleList(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (_output_layer): Sequential(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87863a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
